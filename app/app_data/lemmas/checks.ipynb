{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-29T10:54:45.903041Z",
     "start_time": "2024-12-29T10:54:45.450312Z"
    }
   },
   "id": "56ed01a133c58291"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "265,英国周五表示，关于申请成为中国支持的亚洲基础设施投资银行（AIIB）的创始成员国的决定符合英国的国家利益，对美国对英国上述决定表示担心做出了回应。,英国可能成为第一个支持中国牵头的亚洲开发银行的西方国家。美国对此表示担忧，但希望英国能促进亚投行提高管理标准。中国表示亚投行将吸取其他多边发展银行的经验，避免走弯路。亚投行初期将形成500亿美元的资本规模，成为自主亚洲地区基础设施开发的国际金融机构。,\"英国可能成为首个加入中国主导的亚投行的西方国家，首相发言人称此举符合英国国家利益。尽管事先与美国财长沟通过，但英国并不认为会损害英美关系。英国相信亚投行将补充现有投资机构，并能确保其高标准管理，否认此举是为了讨好北京。美国则对亚投行的管理能力表示担忧，并游说盟友不要加入。而中国表示亚投行将吸取其他多边银行的经验教训。\n",
    "\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-29T11:16:23.222027Z",
     "start_time": "2024-12-29T11:16:22.885244Z"
    }
   },
   "id": "a2dae21ea2b7a678"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from HanziNLP import sentence_segment\n",
    "tokenized_sentences = sentence_segment(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-29T11:16:45.087225Z",
     "start_time": "2024-12-29T11:16:45.022241Z"
    }
   },
   "id": "bb42170b1baa1b78"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "10"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-29T11:16:51.409569Z",
     "start_time": "2024-12-29T11:16:51.340219Z"
    }
   },
   "id": "6f8a87d3f7bbad20"
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-28T21:18:24.035816Z",
     "start_time": "2024-12-28T21:18:23.966145Z"
    }
   },
   "id": "cc1c3c8cae9dc689"
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "outputs": [
    {
     "data": {
      "text/plain": "['\\nЗа новими банківськими правилами клієнтам треба менше часу, щоб перевести поточний рахунок з одного банку в інший.',\n 'Часто кажуть, що стосунки між банком і клієнтом триваліші за більшість шлюбів.',\n 'Які ж іще стосунки можна назвати довготривалими?',\n '\",\"Понад 50% британських власників рахунків ніколи не змінювали банк, а багато людей залишаються вірними своїм перукарям, стоматологам та постачальникам електроенергії.',\n 'Люди не хочуть ризикувати та витрачати час на пошук нових послуг, навіть якщо це може призвести до економії грошей.',\n '\",\"Більше половини британців ніколи не змінювали банк, а середній термін \"\"стосунків\"\" з банком становить 17 років.',\n 'Люди неохоче змінюють банки через уявну складність процедури, хоча час переведення рахунку скоротився до семи днів.',\n 'Найбільш лояльні клієнти - у перукарів,  через страх ризику та невпевненість у нових спеціалістах.',\n \"Також високий рівень лояльності спостерігається у сфері стоматологічних послуг та мобільного зв'язку.\"]"
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize_sentences(text, 'ukrainian')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-29T10:19:10.284137Z",
     "start_time": "2024-12-29T10:19:10.213818Z"
    }
   },
   "id": "6bef56da2e1103a0"
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "outputs": [],
   "source": [
    "def sent_tokenize_sentences(text: str, language: str) -> str:\n",
    "    if language == 'chinese' or language == 'japanese':\n",
    "        from HanziNLP import sentence_segment\n",
    "        tokenized_sentences = sentence_segment(text)\n",
    "    else:\n",
    "        from nltk.tokenize import sent_tokenize\n",
    "        tokenized_sentences = sent_tokenize(text)\n",
    "    return tokenized_sentences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-28T21:33:09.430686Z",
     "start_time": "2024-12-28T21:33:09.371257Z"
    }
   },
   "id": "2c788d07b152a939"
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "outputs": [],
   "source": [
    "def word_tokenize_sentence(sentence: str, language: str) -> str:\n",
    "        if language == 'chinese' or language == 'japanese':\n",
    "            from HanziNLP import word_tokenize\n",
    "            tokenized_sentence = word_tokenize(sentence)\n",
    "        else:\n",
    "            from nltk.tokenize import word_tokenize\n",
    "            tokenized_sentence = word_tokenize(sentence)\n",
    "        return tokenized_sentence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-29T10:17:57.219017Z",
     "start_time": "2024-12-29T10:17:57.150120Z"
    }
   },
   "id": "5d93a04e7ffb8543"
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "outputs": [],
   "source": [
    "import fugashi\n",
    "tagger = fugashi.Tagger()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-29T10:17:45.370964Z",
     "start_time": "2024-12-29T10:17:45.245874Z"
    }
   },
   "id": "d601d3eabb7cc9e7"
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "tagged_words_str = list(map(str, tagged_words))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-28T18:47:15.995244Z",
     "start_time": "2024-12-28T18:47:15.936960Z"
    }
   },
   "id": "66363d56c73c62e1"
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "encoding without a string argument",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[372], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtagger\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtagged_words_str\u001B[49m\u001B[43m)\u001B[49m \n",
      "File \u001B[0;32mfugashi/fugashi.pyx:238\u001B[0m, in \u001B[0;36mfugashi.fugashi.GenericTagger.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mfugashi/fugashi.pyx:254\u001B[0m, in \u001B[0;36mfugashi.fugashi.GenericTagger.parseToNodeList\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: encoding without a string argument"
     ]
    }
   ],
   "source": [
    "tagger(tagged_words_str) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-29T10:17:51.988006Z",
     "start_time": "2024-12-29T10:17:51.842905Z"
    }
   },
   "id": "1bc428af53a93d4d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b3fe5735746e512"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from app.app_data.lemmas.lemmatizers import LANGUAGE_TO_lemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T18:18:13.074720Z",
     "start_time": "2024-12-30T18:18:13.058133Z"
    }
   },
   "id": "67b1cd9654df764f"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "heb = LANGUAGE_TO_lemmatizer['arabic']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T18:22:31.915391Z",
     "start_time": "2024-12-30T18:22:31.899648Z"
    }
   },
   "id": "fce4d81372216a57"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "'رغم'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heb(\"ورغم ردود الفعل كانت هناك\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T18:22:58.431968Z",
     "start_time": "2024-12-30T18:22:57.348258Z"
    }
   },
   "id": "2cce7893df9a69ef"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import qalsadi\n",
    "_arabic_parser = None\n",
    "def _get_arabic_parser():\n",
    "    global _arabic_parser\n",
    "    if _arabic_parser is None:  # Initialize only if not already cached\n",
    "        _arabic_parser = qalsadi.lemmatizer.Lemmatizer()\n",
    "    return _arabic_parser\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T18:23:37.018731Z",
     "start_time": "2024-12-30T18:23:37.000733Z"
    }
   },
   "id": "1d5cd5de6ec0b2b9"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "arabic_model = _get_arabic_parser()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T18:23:42.516871Z",
     "start_time": "2024-12-30T18:23:41.732253Z"
    }
   },
   "id": "486f294e990bb436"
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "human = \"\"\"\n",
    "אתר \"גלובס\" פירסם שח\"כ מיכל בירן מחפשת זוגיות באתר ההיכרויות גיידייט. את תצלום הפרופיל שלחה בירן עצמה. העוזר של בירן: היא סיפקה את התמונה אחרי שהכתבת הודיעה שהיא מתכוונת לכתוב על כך\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T18:44:06.726836Z",
     "start_time": "2024-12-30T18:44:06.708057Z"
    }
   },
   "id": "2a19c3621b286175"
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "sum1 = \"\"\". אתר \"גלובס\" פרסם כתבה על כך שכללה צילומסך מפרופיל ההיכרויות שלה, אך הצילומסך היה אולם שבירן רואה אותו ולא כפי שמשתמשים אחרים רואים אותו. הדבר רמז שבירן סיפקה את הצילומסך לכתבת.\n",
    "בת ה-35 חיפשה זוגיות  \"\"\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T18:44:06.903970Z",
     "start_time": "2024-12-30T18:44:06.884640Z"
    }
   },
   "id": "943feea808cb32f8"
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "{'rouge1': Score(precision=0.2222222222222222, recall=0.24242424242424243, fmeasure=0.2318840579710145),\n 'rouge2': Score(precision=0.08571428571428572, recall=0.09375, fmeasure=0.08955223880597014),\n 'rouge3': Score(precision=0.0, recall=0.0, fmeasure=0.0),\n 'rougeL': Score(precision=0.1388888888888889, recall=0.15151515151515152, fmeasure=0.14492753623188404)}"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rouge3', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(human,\n",
    "                      sum1)\n",
    "scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T18:44:07.090148Z",
     "start_time": "2024-12-30T18:44:07.071718Z"
    }
   },
   "id": "7bceec344da1fbc2"
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def replace_conjunctions(text, language: str, num_replacements=None):\n",
    "    \"\"\"\n",
    "    This function searches for words in the input text that match any word\n",
    "    from the provided conjunctions list and replaces them with a random word\n",
    "    from the same list, up to a specified number of replacements.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The input text to search and replace conjunctions in.\n",
    "    - conjunctions (list, optional): List of words to search and replace. If None, defaults to the provided list.\n",
    "    - num_replacements (int, optional): The number of words to replace. If None, replaces all occurrences.\n",
    "\n",
    "    Returns:\n",
    "    - str: The modified text with conjunctions replaced.\n",
    "    \"\"\"\n",
    "    # Default list of conjunctions if not provided\n",
    "    \n",
    "    conjunctions = L\n",
    "    if conjunctions is None:\n",
    "        conjunctions = [\n",
    "            \"abi\", \"amọ\", \"ati\", \"bi\", \"dẹ\", \"ko\", \"pe\", \"pẹlu\", \"si\", \"ṣugbọn\", \"tabi\", \"yala\"\n",
    "        ]\n",
    "    \n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Counter for number of replacements made\n",
    "    replacements_made = 0\n",
    "    \n",
    "    # Iterate over the words and replace conjunctions\n",
    "    for i, word in enumerate(words):\n",
    "        if word in conjunctions:\n",
    "            if num_replacements is not None and replacements_made >= num_replacements:\n",
    "                break  # Stop replacing if the max number of replacements is reached\n",
    "            \n",
    "            # Replace with a random word from the list (excluding the current word)\n",
    "            replacement = random.choice([w for w in conjunctions if w != word])\n",
    "            words[i] = replacement\n",
    "            replacements_made += 1  # Increment the replacement counter\n",
    "    \n",
    "    # Join the words back into a string\n",
    "    return ' '.join(words)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T19:25:53.352795Z",
     "start_time": "2024-12-30T19:25:53.290172Z"
    }
   },
   "id": "b880454dc1db1613"
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Abi o mọ pe ko dara. Tabi o fẹ jẹ si.\n",
      "Modified text: Abi o mọ pẹlu ko dara. Tabi o fẹ jẹ si.\n"
     ]
    }
   ],
   "source": [
    "text = \"Abi o mọ pe ko dara. Tabi o fẹ jẹ si.\"\n",
    "\n",
    "# Replace only 2 conjunctions\n",
    "new_text = replace_conjunctions(text, num_replacements=1)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Modified text:\", new_text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T19:26:11.520834Z",
     "start_time": "2024-12-30T19:26:11.505066Z"
    }
   },
   "id": "115ca83e016fa4ee"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Lemmatizer' object has no attribute 'parse'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43marabic_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mورغم ردود الفعل كانت هناك\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Lemmatizer' object has no attribute 'parse'"
     ]
    }
   ],
   "source": [
    "arabic_model.parse(\"ورغم ردود الفعل كانت هناك\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T18:24:07.239375Z",
     "start_time": "2024-12-30T18:24:07.230523Z"
    }
   },
   "id": "4dd8e0033f705ee9"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "_dicta_parser_cache = None\n",
    "from app.app_data.lemmas.dikta_parser import DictaParser\n",
    "\n",
    "def _get_dicta_parser():\n",
    "    global _dicta_parser_cache\n",
    "    if _dicta_parser_cache is None:  # Initialize only if not already cached\n",
    "        _dicta_parser_cache = DictaParser()\n",
    "    return _dicta_parser_cache\n",
    "\n",
    "\n",
    "def lemmatize_hebrew(word: str) -> str:\n",
    "    d = _get_dicta_parser()\n",
    "    result_dikta = d.parse(word)\n",
    "    ud_trees = result_dikta[\"ud_trees\"]\n",
    "    result = []\n",
    "    for ud_tree in ud_trees:\n",
    "        key = \"FORM\" if not True else \"LEMMA\"\n",
    "        lemmas = [\n",
    "            x[key] for x in ud_tree if x[key] != \"_\" and x[\"UPOS\"] in (\"NOUN\", \"VERB\")\n",
    "        ]\n",
    "        result += lemmas\n",
    "    return result[0] if len(result) > 0 else None\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T18:19:38.641192Z",
     "start_time": "2024-12-30T18:19:38.629073Z"
    }
   },
   "id": "c6961d6d22de5b14"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "    d = _get_dicta_parser()\n",
    "    result_dikta = d.parse(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T18:19:56.870608Z",
     "start_time": "2024-12-30T18:19:56.441784Z"
    }
   },
   "id": "1ad4a843a0cd5424"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "{'ud_trees': [[{'ID': '1',\n    'FORM': 'למרות',\n    'LEMMA': 'למרות',\n    'UPOS': 'ADP',\n    'XPOS': 'ADP',\n    'FEATS': '',\n    'HEAD': '3',\n    'DEPREL': 'case',\n    'DEPS': '_',\n    'MISC': '_',\n    'sentence': 0},\n   {'ID': '2-3',\n    'FORM': 'הבעיות',\n    'LEMMA': '_',\n    'UPOS': '_',\n    'XPOS': '_',\n    'FEATS': '_',\n    'HEAD': '_',\n    'DEPREL': '_',\n    'DEPS': '_',\n    'MISC': '_',\n    'sentence': 0},\n   {'ID': '2',\n    'FORM': 'ה',\n    'LEMMA': 'ה',\n    'UPOS': 'DET',\n    'XPOS': 'DET',\n    'FEATS': '_',\n    'HEAD': '3',\n    'DEPREL': 'det',\n    'DEPS': '_',\n    'MISC': '_',\n    'sentence': 0},\n   {'ID': '3',\n    'FORM': 'בעיות',\n    'LEMMA': 'בעיה',\n    'UPOS': 'NOUN',\n    'XPOS': 'NOUN',\n    'FEATS': 'Gender=Fem|Number=Plur',\n    'HEAD': '0',\n    'DEPREL': 'root',\n    'DEPS': '_',\n    'MISC': '_',\n    'sentence': 0},\n   {'ID': '4-5',\n    'FORM': 'שהיו',\n    'LEMMA': '_',\n    'UPOS': '_',\n    'XPOS': '_',\n    'FEATS': '_',\n    'HEAD': '_',\n    'DEPREL': '_',\n    'DEPS': '_',\n    'MISC': '_',\n    'sentence': 0},\n   {'ID': '4',\n    'FORM': 'ש',\n    'LEMMA': 'ש',\n    'UPOS': 'SCONJ',\n    'XPOS': 'SCONJ',\n    'FEATS': '_',\n    'HEAD': '3',\n    'DEPREL': 'mark',\n    'DEPS': '_',\n    'MISC': '_',\n    'sentence': 0},\n   {'ID': '5',\n    'FORM': 'היו',\n    'LEMMA': 'היי',\n    'UPOS': 'AUX',\n    'XPOS': 'AUX',\n    'FEATS': 'Gender=Fem|Number=Plur|Person=3|Tense=Past',\n    'HEAD': '3',\n    'DEPREL': 'acl:relcl',\n    'DEPS': '_',\n    'MISC': '_',\n    'sentence': 0}],\n  []],\n 'json_datas': [{'text': 'למרות הבעיות שהיו',\n   'tokens': [{'token': 'למרות',\n     'offsets': {'start': 0, 'end': 5},\n     'syntax': {'word': 'למרות',\n      'dep_head_idx': 1,\n      'dep_func': 'case',\n      'dep_head': 'הבעיות'},\n     'seg': ('למרות',),\n     'lex': 'למרות',\n     'morph': {'token': 'למרות',\n      'pos': 'ADP',\n      'feats': {},\n      'prefixes': [],\n      'suffix': False},\n     'sentence': '0'},\n    {'token': 'הבעיות',\n     'offsets': {'start': 6, 'end': 12},\n     'syntax': {'word': 'הבעיות',\n      'dep_head_idx': -1,\n      'dep_func': 'root',\n      'dep_head': 'שהיו'},\n     'seg': ('ה', 'בעיות'),\n     'lex': 'בעיה',\n     'morph': {'token': 'הבעיות',\n      'pos': 'NOUN',\n      'feats': {'Gender': 'Fem', 'Number': 'Plur'},\n      'prefixes': ['DET'],\n      'suffix': False},\n     'sentence': '0'},\n    {'token': 'שהיו',\n     'offsets': {'start': 13, 'end': 17},\n     'syntax': {'word': 'שהיו',\n      'dep_head_idx': 1,\n      'dep_func': 'acl:relcl',\n      'dep_head': 'הבעיות'},\n     'seg': ('ש', 'היו'),\n     'lex': 'היי',\n     'morph': {'token': 'שהיו',\n      'pos': 'AUX',\n      'feats': {'Gender': 'Fem',\n       'Number': 'Plur',\n       'Person': '3',\n       'Tense': 'Past'},\n      'prefixes': ['SCONJ'],\n      'suffix': False},\n     'sentence': '0'}],\n   'root_idx': 1,\n   'ner_entities': []},\n  {'text': '', 'tokens': [], 'root_idx': 0, 'ner_entities': []}]}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.parse(\"למרות הבעיות שהיו\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T18:21:17.439266Z",
     "start_time": "2024-12-30T18:21:17.101820Z"
    }
   },
   "id": "d8a1faf93ceca95c"
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "outputs": [],
   "source": [
    "tokenized_sentences = sent_tokenize(text)\n",
    "words_tokenized = [word_tokenize(sent) for sent in tokenized_sentences]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-28T21:34:20.848599Z",
     "start_time": "2024-12-28T21:34:20.746575Z"
    }
   },
   "id": "cb368aa913a6578c"
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "outputs": [
    {
     "data": {
      "text/plain": "['37',\n ',',\n 'На',\n 'сайті',\n 'Міністерства',\n 'внутрішніх',\n 'справ',\n 'України',\n 'запустили',\n 'сервіс',\n 'для',\n 'пошуку',\n 'викрадених',\n 'паспортів',\n '.']"
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(tokenized_sentences[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-29T10:16:53.970703Z",
     "start_time": "2024-12-29T10:16:53.462823Z"
    }
   },
   "id": "e6acfc2e8980be67"
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "131,\"כשברקע קנס של 6 מיליון שקלים, ב\"\"נשר\"\" התרחשו עשרות תקלות בחודשים האחרונים • יעל געתון הסבירה בכאן 11 על מעקב \"\"שקוף\"\" אחר הנושא שנמצא מתחת לרדאר הציבורי\",\"במפעל המלט \"\"נשר\"\" ברמלה קרו 26 תקלות בחודשיים האחרונים. המפעל הוא המזהם ביותר בישראל והמשרד להגנת הסביבה קנס אותו ב-6 מיליון שקלים. יעל געתון ויפעת גליק דיברו על הנושא בתוכנית \"\"הזמן הירוק\"\" בכאן 11. הם חשפו את המעקב \"\"שקוף\"\" שלא קיבל חשיפה רבה בציבור ומשפיע על תושבי רמלה והסביבה.\",\"מפעל המלט \"\"נשר\"\" ברמלה סובל מריבוי תקלות, כאשר 26 תקלות תועדו בחודשיים האחרונים.  המשרד להגנת הסביבה קנס את המפעל ב-6 מיליון שקלים בשל היותו המפעל הפרטי המזהם ביותר בישראל.  יעל געתון התראיינה בתוכנית \"\"הזמן הירוק\"\" ודנה במעקב של \"\"שקוף\"\" אחר זיהום המפעל.  הזיהום משפיע על תושבי רמלה והסביבה אך לא זוכה לתשומת לב ציבורית מספקת.  המעקב של \"\"שקוף\"\" נועד להגביר את המודעות לבעיה.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T20:06:25.251172Z",
     "start_time": "2024-12-30T20:06:25.123050Z"
    }
   },
   "id": "c299bbcee778d9e8"
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [],
   "source": [
    "tokenized_sentences = sent_tokenize(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T20:06:57.022628Z",
     "start_time": "2024-12-30T20:06:56.973372Z"
    }
   },
   "id": "20e617365dd484e0"
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "data": {
      "text/plain": "['\\n131,\"כשברקע קנס של 6 מיליון שקלים, ב\"\"נשר\"\" התרחשו עשרות תקלות בחודשים האחרונים • יעל געתון הסבירה בכאן 11 על מעקב \"\"שקוף\"\" אחר הנושא שנמצא מתחת לרדאר הציבורי\",\"במפעל המלט \"\"נשר\"\" ברמלה קרו 26 תקלות בחודשיים האחרונים.',\n 'המפעל הוא המזהם ביותר בישראל והמשרד להגנת הסביבה קנס אותו ב-6 מיליון שקלים.',\n 'יעל געתון ויפעת גליק דיברו על הנושא בתוכנית \"\"הזמן הירוק\"\" בכאן 11.',\n 'הם חשפו את המעקב \"\"שקוף\"\" שלא קיבל חשיפה רבה בציבור ומשפיע על תושבי רמלה והסביבה.',\n '\",\"מפעל המלט \"\"נשר\"\" ברמלה סובל מריבוי תקלות, כאשר 26 תקלות תועדו בחודשיים האחרונים.',\n 'המשרד להגנת הסביבה קנס את המפעל ב-6 מיליון שקלים בשל היותו המפעל הפרטי המזהם ביותר בישראל.',\n 'יעל געתון התראיינה בתוכנית \"\"הזמן הירוק\"\" ודנה במעקב של \"\"שקוף\"\" אחר זיהום המפעל.',\n 'הזיהום משפיע על תושבי רמלה והסביבה אך לא זוכה לתשומת לב ציבורית מספקת.',\n 'המעקב של \"\"שקוף\"\" נועד להגביר את המודעות לבעיה.']"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T20:07:01.905259Z",
     "start_time": "2024-12-30T20:07:01.833379Z"
    }
   },
   "id": "890f1dabebeecbe1"
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [
    {
     "data": {
      "text/plain": "{'למה שהוועדה שבה כל מה שקורה בכנסת מוכרע בה – לא תהיה פתוחה לסיקור עיתונאי הוגן?'}"
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(new_tokenied) - set(tokenized_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T20:09:09.259053Z",
     "start_time": "2024-12-30T20:09:09.231003Z"
    }
   },
   "id": "4a1a205fba61e66d"
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "new_tokenied = insert_random_sentence(tokenized_sentences, \"hebrew\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T20:09:08.141131Z",
     "start_time": "2024-12-30T20:09:07.900661Z"
    }
   },
   "id": "510aa5f48863b8dd"
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "\n",
    "def insert_random_sentence(tokenized_sentences: List[str], language: str):\n",
    "    tokenized_sentences_copy = copy.deepcopy(tokenized_sentences)\n",
    "    data = pd.read_csv(f\"/Users/itaimondshine/PycharmProjects/NLP/eval_metrics/app/app_data/xlsum/{language}.csv\")\n",
    "    tokenized_text = sent_tokenize(data.sample().iloc[0]['text'])\n",
    "    selected_sentence = tokenized_text[random.randint(0, len(tokenized_text) -1 )]\n",
    "    random_index = random.randint(0, len(tokenized_sentences))  # This allows insertion at the end as well\n",
    "    tokenized_sentences_copy.insert(random_index, selected_sentence)\n",
    "    return tokenized_sentences_copy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T20:09:16.450196Z",
     "start_time": "2024-12-30T20:09:16.423085Z"
    }
   },
   "id": "a62a6d1c6a9c9227"
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "language = 'spanish'\n",
    "data = pd.read_csv(f\"/Users/itaimondshine/PycharmProjects/NLP/eval_metrics/app/app_data/xlsum/{language}.csv\")\n",
    "tokenized_text = sent_tokenize(data.sample().iloc[0]['text'])\n",
    "tokenized_text[random.randint(0, len(tokenized_text) -1 )]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T20:00:15.652342Z",
     "start_time": "2024-12-30T20:00:15.334749Z"
    }
   },
   "id": "f3310d43eb1d1b0c"
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "data": {
      "text/plain": "'Antes de salir a correr es recomendable saber qué alimentos son los más adecuados para mejorar el rendimiento. Cada alimento que se ingiere tiene un efecto determinado en el organismo capaz de impulsar ciertas reacciones y anular otras, tanto si se desea una tarde de ocio o un día de entrenamiento físico. Por eso, si eres una de las millones de personas alrededor del mundo que sale a correr al menos una vez por semana es recomendable plantearte algunas preguntas. Las personas que corren casualmente notarán rápidamente la diferencia de cómo reacciona el cuerpo con una buena alimentación. ¿Piensas qué comes antes de salir por la puerta para estirar las piernas? ¿Tomas la bebida adecuada para recuperarte cuando regresas del ejercicio? Sea para salir a dar una vuelta al parque un sábado en la mañana o para entrenar de cara a pruebas de 10 kilómetros o maratones, el alimento adecuado puede marcar una gran diferencia en tu rendimiento. Final de Quizás también te interese Kilos y kilómetros La nutricionista deportiva Renee McGregor desglosó para la BBC una serie de recomendaciones sobre los alimentos que se deben consumir, basado en grupos alimentarios, edad y género de las personas. Sobre los primeros, McGregor resalta la importancia para el ejercicio de los carbohidratos y las proteínas. Los atletas de élite requieren de mucha más cantidad de proteína e ir recuperando constantemente los carbohidratos consumidos. Los carbohidratos proveen de energía a los músculos, el problema es que el cuerpo humano sólo puede almacenar suficiente glucógeno para correr entre 60 y 90 minutos, dependiendo de la intensidad y la habilidad del corredor. Cuando estas reservas se agotan es cuando las personas sienten como pierden toda su energía. Lo importante es obtener la cantidad necesaria de carbohidratos sin sobrecargar el estómago. Lea: 5 mitos de la nutrición en el deporte Después de un desayuno en el que se puede combinar avena, tostadas integrales y huevos revueltos, unas dos horas antes de salir a correr, es necesario mantener al cuerpo con los niveles de carbohidratos recomendados. Dependiendo de la distancia el cuerpo necesitará entre 30 y 60 gramos por hora que se pueden suministrar con una banana (25g) o una bebida elaborada en casa de 300ml de fruta, 200ml de agua y un cuarto de cucharadita de sal (30g). Los dulces de jalean pueden contener 5g. 1. Remolacha Recientes estudios de la Universidad de Exeter muestran que la remolacha puede mejorar el rendimiento de los atletas debido a su alto contenido de nitrato. Este compuesto se transforma en óxido de nitrógeno en el cuerpo, lo que puede reducir la cantidad de energía y oxígeno que se consume durante el ejercicio, lo que permite a los atletas correr más rápido y durante más tiempo. Es probable que las personas que corren casualmente perciban más diferencias que los deportistas que entrenan regularmente. Dosis: Al menos 5mmol (milimoles por litro), de dos a tres horas antes de entrenar durante cinco días antes de una carrera. Cada uno de estos alimentos contribuyen a correr más rápido y durante más tiempo. 2. Granos integrales Para conseguir un ritmo rápido de manera constante las personas necesitan contar con reservas de carbohidratos en su cuerpo. Este grupo alimentario es clave como fuente del ejercicio ya que se transforma en glucosa y se utiliza para proveer energía a los músculos. Sólo se puede almacenar cantidad limitadas de carbohidratos en forma de glucógeno en el hígado y los músculos, por lo que es importante aprovechar ese espacio.Dosis: es preferible escoger alimentos ricos en carbohidratos de avena o granos integrales. 3. Calcio El calcio es el que desarrolla y mantiene la densidad de los huesos y previene lesiones. Es recomendable que adultos físicamente activos y niños mayores de nueve años consuman 1300mg de calcio al día. Los lácteos es la fuente más común, pero hay otras opciones como los productos de soja, vegetales verdes como las espinacas o el brócoli o pequeños peces como anchoas y sardinas. Dosis: la cantidad necesaria de 1300mg es posible obtenerla con un pequeño pedazo de queso, 250ml de leche, tres sardinas, 100 gramos de tofu o un pote pequeño de yogurt. La cantidad necesaria de proteína debería caber en la palma de una mano. En cuanto a las proteínas estas son importantes ya que ayudan a desarrollar, reparar y recuperar los músculos, pero la cantidad que se consuma dependerá de cuánto ejercicio se haga. Para un corredor casual se sugiere unos 0,8 gramos para las mujeres y 1 gramo para los hombres por kilo del cuerpo. Si una mujer pesa 57 kilos, por ejemplo, necesitara de 46 gramos al día, que pueden lograrse con dos huevos, 75 gramos de pollo y 400ml de leche. Atletas regulares deben consumir unos 0,25 gramos por kilo de su cuerpo de unas tres a seis veces al día. Lea: ¿Están los vegetarianos en desventaja deportiva? Lea: ¿Se puede ser vegano y deportista exitoso? Tanto las personas vegetarianas como los veganos pueden obtener estas cantidades en una variedad de productos. Edad y género Los atletas jóvenes necesitarán de alimentos suplementarios para poder tener suficiente energía para hacer ejercicio y crecer. Batidos de leches o de frutas y sánguches son buenas opciones. Los huevos, que se pueden consumir de muchas formas, ofrecen una buena cantidad de proteínas para el organismo. Las cantidades de calcio de 1300mg se deben mantener incluso entre los adultos ya que la densidad de los huesos puede comenzar a disminuir a partir de los 25 años de edad. Para los mayores de 50 años también es importante aumentar el consumo de proteínas para mantener la masa muscular. Las mujeres deben tener cuidado con el ciclo menstrual ya que puede afectar el rendimiento entre los días 1 y 13 cuando los niveles de estrógeno aumenta, que hace que el cuerpo femenino consuma más porcentaje de grasa y energía. La alimentación en las mujeres puede variar dependiendo del ciclo menstrual. Cuando esta fase pasa, comienza a haber un aumento de progesterona que hace al cuerpo más dependiente de los carbohidratos. Lea: El tabú de la menstruación en las deportistas profesionales La progesterona también causa que la temperatura del cuerpo aumente, lo que hace que la mujer se sienta más hambrienta entre los días 16-28 del ciclo. Para después del ejercicio es recomendable una bebida que contenga carbohidrato (para recuperar las reservas de glucógena), calcio (para la salud de los huesos) y proteína (para reconstruir y reparar los músculos). La opción preferida puede ser un batido de chocolate.'"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample().iloc[0]['text']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T20:01:02.181949Z",
     "start_time": "2024-12-30T20:01:02.164717Z"
    }
   },
   "id": "3c53dfd667f18224"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random.randint(0, len(text) -1 )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "381f44f9f2a91e28"
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "data": {
      "text/plain": "'\"Allí las autoridades pudieron contener la infección controlando la cantidad de mosquitos\".'"
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = sent_tokenize(data.sample().iloc[0]['text'])\n",
    "tokenized_text[random.randint(0, len(tokenized_text) -1 )]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T20:02:50.751442Z",
     "start_time": "2024-12-30T20:02:50.733773Z"
    }
   },
   "id": "844eeb7e93fee981"
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "lemmatize = LANGUAGE_TO_lemmatizer['ukrainian']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-30T19:59:07.727544Z",
     "start_time": "2024-12-30T19:59:07.700058Z"
    }
   },
   "id": "330b68dd556fb6df"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "_indonesian_parser = None\n",
    "def _get_indonesian_parser():\n",
    "    global _indonesian_parser\n",
    "    if _indonesian_parser is None:  # Initialize only if not already cached\n",
    "        from nlp_id.lemmatizer import Lemmatizer \n",
    "        _indonesian_parser = Lemmatizer()\n",
    "    return _indonesian_parser\n",
    "\n",
    "def lemmatize_indonesian(word: str) -> str:\n",
    "    lemmatizer = _get_indonesian_parser()\n",
    "    return lemmatizer.lemmatize(word)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-29T10:32:19.084024Z",
     "start_time": "2024-12-29T10:32:19.060923Z"
    }
   },
   "id": "ee5e4e3b5c8aa007"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "lemmatizer = _get_indonesian_parser()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-29T10:34:43.533731Z",
     "start_time": "2024-12-29T10:34:43.509910Z"
    }
   },
   "id": "efc4ddc06d62743a"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "'henti'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"menghentikan\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-29T10:35:03.414244Z",
     "start_time": "2024-12-29T10:35:03.397665Z"
    }
   },
   "id": "76c16ace4d6c6482"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "''"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "|lemmatize_indonesian(\"довготривалими\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-29T10:32:20.593661Z",
     "start_time": "2024-12-29T10:32:19.691550Z"
    }
   },
   "id": "8cf401c776281601"
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-29T10:19:23.172875Z",
     "start_time": "2024-12-29T10:19:22.975824Z"
    }
   },
   "id": "4d6593505e3d2e77"
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "outputs": [
    {
     "data": {
      "text/plain": "[軍, に, おけ, る, 性的, 暴行, 問題, を, 女性, の, 入隊, と, 結びつける, 持論, を, 改めて, 主張, し, た]"
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger(\"軍における性的暴行問題を女性の入隊と結びつける持論を改めて主張した\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-29T10:18:15.574117Z",
     "start_time": "2024-12-29T10:18:15.477641Z"
    }
   },
   "id": "197f22d6f14145d0"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "_japanese_parser = None\n",
    "\n",
    "def _get_japanese_parser():\n",
    "    global _japanese_parser\n",
    "    if _japanese_parser is None:  # Initialize only if not already cached\n",
    "        _japanese_parser = spacy.load(\"ja_core_news_sm\")\n",
    "    return _japanese_parser"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-28T18:26:41.336510Z",
     "start_time": "2024-12-28T18:26:41.278222Z"
    }
   },
   "id": "c7ce480e16c28596"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "nlp = _get_japanese_parser()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-28T18:27:00.407264Z",
     "start_time": "2024-12-28T18:26:59.885035Z"
    }
   },
   "id": "57fdb865084aba39"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# _spanish_parser = None\n",
    "# \n",
    "# def _get_spanish_parser():\n",
    "#     global _spanish_parser\n",
    "#     if _spanish_parser is None:  # Initialize only if not already cached\n",
    "#         _spanish_parser = spacy.load(\"es_core_news_sm\")\n",
    "#     return _spanish_parser\n",
    "# \n",
    "# \n",
    "# def lemmatize_spanish(word: str) -> str:\n",
    "#     nlp = _get_spanish_parser()\n",
    "#     return nlp(word)[0].lemma_\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-28T17:25:11.185132Z",
     "start_time": "2024-12-28T17:25:11.129163Z"
    }
   },
   "id": "4033cd01043f42df"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "# sentence = sent_tokenize(sample)\n",
    "# words = word_tokenize(sentence)\n",
    "import random\n",
    "import copy\n",
    "from typing import List, Union"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-28T18:23:39.697092Z",
     "start_time": "2024-12-28T18:23:39.398174Z"
    }
   },
   "id": "a400dc23e75c6513"
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "outputs": [],
   "source": [
    "def replace_random_words_with_lemmas(tokenized_words: Union[List[str], List[List[str]]], num_to_replace: int) -> Union[List[str], List[List[str]]]:\n",
    "    words = copy.deepcopy(tokenized_words)\n",
    "    punctuation_marks = {\".\", \"?\", \"!\", '``', \"''\", ','}\n",
    "    all_indices = [(i, j) for i, sublist in enumerate(words) for j, word in enumerate(sublist) if word not in punctuation_marks and not isinstance(word, int)]\n",
    "    indices_to_replace = random.sample(all_indices, min(num_to_replace, len(all_indices)))\n",
    "    for i, j in indices_to_replace:\n",
    "        lemmatized_word = lemmatize(words[i][j])\n",
    "        words[i][j] = lemmatized_word if lemmatized_word is not None else words[i][j]\n",
    "        if lemmatized_word:\n",
    "            print(lemmatized_word)\n",
    "    return words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-28T21:28:18.612427Z",
     "start_time": "2024-12-28T21:28:18.532398Z"
    }
   },
   "id": "9997e9286db5839a"
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "outputs": [],
   "source": [
    "tokenized_sentences = sent_tokenize(text)\n",
    "words_tokenized = [word_tokenize(sent) for sent in tokenized_sentences]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-28T21:28:26.417729Z",
     "start_time": "2024-12-28T21:28:26.284012Z"
    }
   },
   "id": "df055f0d01a49e53"
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\"ידיעות אחרונות\"\" מתנצל על שהוסיף ספורטאי למשלחת למינכן | מרדכי גילת נוזף בקולגות | \"\"מעריב\"\" ו\"\"הארץ\"\" מגלים את ביגפוט\",\"עיתוני הבוקר מציגים תוצאות תקשורתיות חלשות, כל עיתון בוחר בכותרת ראשית נושא אחר. הסופר דויד גרוסמן הצטרף לפעילות נשות מחסום Watch והופיע בשער \"\"מעריב\"\". עמוד שלם ב\"\"ידיעות אחרונות\"\" מוקדש למכתבים שכתבו ילדים עם מוגבלויות במחאה על ביטול הסייעות הצמודות להם. בכתבה ב\"\"ידיעות אחרונות\"\" מתנצלים על טעות בכתיבת השם של אילן לביא ז\"\"ל, שנפטר בשבוע שעבר. ידיעה על מציאת ביגפוט מצאה דרכה לעיתונים \"\"הארץ\"\", \"\"מעריב\"\" ו\"\"ידיעות אחרונות\"\", כאשר כל עיתון מציג נקודת מבט שונה בנושא. עו\"\"ד דידי לחמן-מסר עדיין לא נמצא לו מחליף בתפקיד המשנה ליועץ המשפטי לממשלה לעניינים כלכליים-פיסקליים.\"\n",
    "\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-01T21:40:29.871294Z",
     "start_time": "2025-01-01T21:40:29.797987Z"
    }
   },
   "id": "62d4f7902be1f3c0"
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [],
   "source": [
    "text2  = \"\"\"\n",
    " gudang di Izmir yang memproduksi jaket pelampung palsu untuk pengungsi.  Jaket-jaket tersebut diisi denga\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-01T21:44:20.296471Z",
     "start_time": "2025-01-01T21:44:20.245871Z"
    }
   },
   "id": "70b5aa6dbe0ac852"
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer as Detok\n",
    "detokenizer = Detok()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-01T21:45:35.727830Z",
     "start_time": "2025-01-01T21:45:35.692396Z"
    }
   },
   "id": "46fa962b82c22c9f"
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[226], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[43msent_tokenize_sentences\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mturkish\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m text\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/eval_metrics/app/app_data/data/utils.py:26\u001B[0m, in \u001B[0;36msent_tokenize_sentences\u001B[0;34m(text, language)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenize\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sent_tokenize\n\u001B[0;32m---> 26\u001B[0m     tokenized_sentences \u001B[38;5;241m=\u001B[39m \u001B[43msent_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tokenized_sentences\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/nltk/tokenize/__init__.py:120\u001B[0m, in \u001B[0;36msent_tokenize\u001B[0;34m(text, language)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001B[39;00m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;124;03musing NLTK's recommended sentence tokenizer\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;124;03m:param language: the model name in the Punkt corpus\u001B[39;00m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    119\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m _get_punkt_tokenizer(language)\n\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1280\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer.tokenize\u001B[0;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[1;32m   1276\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtokenize\u001B[39m(\u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m, realign_boundaries: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m   1277\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1278\u001B[0m \u001B[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001B[39;00m\n\u001B[1;32m   1279\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1280\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msentences_from_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrealign_boundaries\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1340\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer.sentences_from_text\u001B[0;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[1;32m   1331\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msentences_from_text\u001B[39m(\n\u001B[1;32m   1332\u001B[0m     \u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m, realign_boundaries: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1333\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m   1334\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1335\u001B[0m \u001B[38;5;124;03m    Given a text, generates the sentences in that text by only\u001B[39;00m\n\u001B[1;32m   1336\u001B[0m \u001B[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001B[39;00m\n\u001B[1;32m   1337\u001B[0m \u001B[38;5;124;03m    True, includes in the sentence closing punctuation that\u001B[39;00m\n\u001B[1;32m   1338\u001B[0m \u001B[38;5;124;03m    follows the period.\u001B[39;00m\n\u001B[1;32m   1339\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1340\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [text[s:e] \u001B[38;5;28;01mfor\u001B[39;00m s, e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1340\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1331\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msentences_from_text\u001B[39m(\n\u001B[1;32m   1332\u001B[0m     \u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m, realign_boundaries: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1333\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m   1334\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1335\u001B[0m \u001B[38;5;124;03m    Given a text, generates the sentences in that text by only\u001B[39;00m\n\u001B[1;32m   1336\u001B[0m \u001B[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001B[39;00m\n\u001B[1;32m   1337\u001B[0m \u001B[38;5;124;03m    True, includes in the sentence closing punctuation that\u001B[39;00m\n\u001B[1;32m   1338\u001B[0m \u001B[38;5;124;03m    follows the period.\u001B[39;00m\n\u001B[1;32m   1339\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1340\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [text[s:e] \u001B[38;5;28;01mfor\u001B[39;00m s, e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1328\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer.span_tokenize\u001B[0;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[1;32m   1326\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m realign_boundaries:\n\u001B[1;32m   1327\u001B[0m     slices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_realign_boundaries(text, slices)\n\u001B[0;32m-> 1328\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sentence \u001B[38;5;129;01min\u001B[39;00m slices:\n\u001B[1;32m   1329\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m (sentence\u001B[38;5;241m.\u001B[39mstart, sentence\u001B[38;5;241m.\u001B[39mstop)\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1457\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer._realign_boundaries\u001B[0;34m(self, text, slices)\u001B[0m\n\u001B[1;32m   1444\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1445\u001B[0m \u001B[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001B[39;00m\n\u001B[1;32m   1446\u001B[0m \u001B[38;5;124;03mshould otherwise be included in the same sentence.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1454\u001B[0m \u001B[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001B[39;00m\n\u001B[1;32m   1455\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1456\u001B[0m realign \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1457\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sentence1, sentence2 \u001B[38;5;129;01min\u001B[39;00m _pair_iter(slices):\n\u001B[1;32m   1458\u001B[0m     sentence1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mslice\u001B[39m(sentence1\u001B[38;5;241m.\u001B[39mstart \u001B[38;5;241m+\u001B[39m realign, sentence1\u001B[38;5;241m.\u001B[39mstop)\n\u001B[1;32m   1459\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m sentence2:\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/nltk/tokenize/punkt.py:321\u001B[0m, in \u001B[0;36m_pair_iter\u001B[0;34m(iterator)\u001B[0m\n\u001B[1;32m    319\u001B[0m iterator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(iterator)\n\u001B[1;32m    320\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 321\u001B[0m     prev \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    322\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1429\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer._slices_from_text\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m   1427\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_slices_from_text\u001B[39m(\u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[\u001B[38;5;28mslice\u001B[39m]:\n\u001B[1;32m   1428\u001B[0m     last_break \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1429\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m match, context \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_match_potential_end_contexts(text):\n\u001B[1;32m   1430\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_contains_sentbreak(context):\n\u001B[1;32m   1431\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mslice\u001B[39m(last_break, match\u001B[38;5;241m.\u001B[39mend())\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1394\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m   1392\u001B[0m previous_slice \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mslice\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m   1393\u001B[0m previous_match \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1394\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m match \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_lang_vars\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mperiod_context_re\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfinditer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1395\u001B[0m     \u001B[38;5;66;03m# Get the slice of the previous word\u001B[39;00m\n\u001B[1;32m   1396\u001B[0m     before_text \u001B[38;5;241m=\u001B[39m text[previous_slice\u001B[38;5;241m.\u001B[39mstop : match\u001B[38;5;241m.\u001B[39mstart()]\n\u001B[1;32m   1397\u001B[0m     index_after_last_space \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_last_whitespace_index(before_text)\n",
      "\u001B[0;31mTypeError\u001B[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "text = sent_tokenize_sentences(text, \"turkish\")\n",
    "text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-01T21:48:47.294082Z",
     "start_time": "2025-01-01T21:48:46.958267Z"
    }
   },
   "id": "76acd9a735227465"
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [],
   "source": [
    "tokenized_sentences = word_tokenize_sentence(tokenized_sentences[0], \"hebrew\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-01T21:49:24.841410Z",
     "start_time": "2025-01-01T21:49:24.788335Z"
    }
   },
   "id": "f062a68ba228c244"
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [
    {
     "data": {
      "text/plain": "['``',\n 'ידיעות',\n 'אחרונות',\n \"''\",\n \"''\",\n 'מתנצל',\n 'על',\n 'שהוסיף',\n 'ספורטאי',\n 'למשלחת',\n 'למינכן',\n '|',\n 'מרדכי',\n 'גילת',\n 'נוזף',\n 'בקולגות',\n '|',\n '``',\n \"''\",\n 'מעריב',\n \"''\",\n \"''\",\n 'ו',\n \"''\",\n \"''\",\n 'הארץ',\n \"''\",\n \"''\",\n 'מגלים',\n 'את',\n 'ביגפוט',\n \"''\",\n ',',\n \"''\",\n 'עיתוני',\n 'הבוקר',\n 'מציגים',\n 'תוצאות',\n 'תקשורתיות',\n 'חלשות',\n ',',\n 'כל',\n 'עיתון',\n 'בוחר',\n 'בכותרת',\n 'ראשית',\n 'נושא',\n 'אחר',\n '.']"
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-01T21:49:25.397134Z",
     "start_time": "2025-01-01T21:49:25.333426Z"
    }
   },
   "id": "e2a329462b567555"
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "|detokenized_text = detokenize_two_levels(tokenized_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-01T21:41:24.622116Z",
     "start_time": "2025-01-01T21:41:24.567950Z"
    }
   },
   "id": "a00554d57a49b6d6"
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "detokenized_text = detokenize_one_level(tokenized_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-01T21:41:04.910191Z",
     "start_time": "2025-01-01T21:41:04.880596Z"
    }
   },
   "id": "605143faa3cbe247"
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "sent = str(tokenized_sentences[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-01T21:42:16.620818Z",
     "start_time": "2025-01-01T21:42:16.593945Z"
    }
   },
   "id": "5360435c2f35ac8c"
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n\"ידיעות אחרונות\"\" מתנצל על שהוסיף ספורטאי למשלחת למינכן | מרדכי גילת נוזף בקולגות | \"\"מעריב\"\" ו\"\"הארץ\"\" מגלים את ביגפוט\",\"עיתוני הבוקר מציגים תוצאות תקשורתיות חלשות, כל עיתון בוחר בכותרת ראשית נושא אחר.'"
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-01T21:42:19.347684Z",
     "start_time": "2025-01-01T21:42:19.314226Z"
    }
   },
   "id": "eb1397db3fd4cbf5"
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "על שהוסיף ספורטאי למשלחת למינכן | מרדכי גילת נוזף בקולגות | \"\"מערי\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-01T21:43:17.152742Z",
     "start_time": "2025-01-01T21:43:17.122803Z"
    }
   },
   "id": "24eb07f62c02b1f4"
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "data": {
      "text/plain": "'ע ל   ש ה ו ס י ף   ס פ ו ר ט א י   ל מ ש ל ח ת   ל מ י נ כ ן   |   מ ר ד כ י   ג י ל ת   נ ו ז ף   ב ק ו ל ג ו ת   |   \" \" מ ע ר י'"
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detokenize_one_level(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-01T21:43:30.233444Z",
     "start_time": "2025-01-01T21:43:30.101481Z"
    }
   },
   "id": "822479f52f19473b"
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/itaimondshine/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'es_core_news_sm' (3.1.0) was trained with spaCy v3.1.0 and may not be 100% compatible with the current version (3.7.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[188], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m detokenized_text \u001B[38;5;241m=\u001B[39m detokenize_two_levels(tokenized_sentences)\n\u001B[1;32m      4\u001B[0m ner_results \u001B[38;5;241m=\u001B[39m LANGUAGE_TO_NER[language](detokenized_text)\n\u001B[0;32m----> 5\u001B[0m text \u001B[38;5;241m=\u001B[39m swap_entities_in_text(detokenized_text, ner_results, n\u001B[38;5;241m=\u001B[39m\u001B[43mconfig\u001B[49m\u001B[38;5;241m.\u001B[39mshould_replace_ner\u001B[38;5;241m.\u001B[39mnumber)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mner\u001B[39m\u001B[38;5;124m\"\u001B[39m, text)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# tokenized_sentences = sent_tokenize_sentences(text, language)\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "from app.app_data.data.utils import *\n",
    "\n",
    "detokenized_text = detokenize_one_level(tokenized_sentences)\n",
    "ner_results = LANGUAGE_TO_NER[language](detokenized_text)\n",
    "text = swap_entities_in_text(detokenized_text, ner_results, n=config.should_replace_ner.number)\n",
    "print(\"ner\", text)\n",
    "# tokenized_sentences = sent_tokenize_sentences(text, language)\n",
    "print(tokenized_sentences)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-01T21:39:44.512809Z",
     "start_time": "2025-01-01T21:39:37.298525Z"
    }
   },
   "id": "429ec791631c2520"
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "outputs": [],
   "source": [
    "def detokenize(tokenized_sentences: str) -> str:\n",
    "    from nltk.tokenize.treebank import TreebankWordDetokenizer as Detok\n",
    "    detokenizer = Detok()\n",
    "    text = detokenizer.detokenize(tokenized_sentences)\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-28T21:31:19.038448Z",
     "start_time": "2024-12-28T21:31:18.905614Z"
    }
   },
   "id": "a6f87b41022a8c2b"
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "outputs": [
    {
     "data": {
      "text/plain": "'Bu verilerin yay I mlanmas ı ndan sonra ş yer sigaran ı n tamamen yasaklanmas ı Ç a ğ r ı lar ı yap ı ld ı ., \" Monash Ü niversitesi uzmanlar ı n ı N yapt I ğ ı ara ş t ı rma, sigara ç enlerin daha fazla hastal ı k izni ald I ğ ı n I ve ş yerlerinde daha az verimli olduklar ı n ı ortaya koydu.'"
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detokenize(lemmatized_words[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-28T21:31:24.631788Z",
     "start_time": "2024-12-28T21:31:24.565166Z"
    }
   },
   "id": "6b65cd6112d00ef1"
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "outputs": [
    {
     "data": {
      "text/plain": "['Bu',\n 'verilerin',\n 'yay',\n 'ı',\n 'mlanmas',\n 'ı',\n 'ndan',\n 'sonra',\n 'ş',\n 'yerlerinde',\n 'sigaran',\n 'ı',\n 'n',\n 'tamamen',\n 'yasaklanmas',\n 'ı',\n 'ç',\n 'a',\n 'ğ',\n 'r',\n 'ı',\n 'lar',\n 'ı',\n 'yap',\n 'ı',\n 'ld',\n 'ı',\n '.',\n ',',\n '\"',\n 'Monash',\n 'Ü',\n 'niversitesi',\n 'uzmanlar',\n 'ı',\n 'n',\n 'ı',\n 'n',\n 'yapt',\n 'ı',\n 'ğ',\n 'ı',\n 'ara',\n 'ş',\n 't',\n 'ı',\n 'rma',\n ',',\n 'sigara',\n 'ç',\n 'enlerin',\n 'daha',\n 'fazla',\n 'hastal',\n 'ı',\n 'k',\n 'izni',\n 'ald',\n 'ı',\n 'ğ',\n 'ı',\n 'n',\n 'ı',\n 've',\n 'ş',\n 'yerlerinde',\n 'daha',\n 'az',\n 'verimli',\n 'olduklar',\n 'ı',\n 'n',\n 'ı',\n 'ortaya',\n 'koydu',\n '.']"
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_tokenized[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-28T21:30:14.717126Z",
     "start_time": "2024-12-28T21:30:14.649730Z"
    }
   },
   "id": "5579f2258ff90281"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "import spacy\n",
    "_spanish_parser = None\n",
    "def _get_spanish_parser():\n",
    "    global _spanish_parser\n",
    "    if _spanish_parser is None:  # Initialize only if not already cached\n",
    "        _spanish_parser = spacy.load(\"es_core_news_md\")\n",
    "    return _spanish_parser\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-28T17:33:28.294208Z",
     "start_time": "2024-12-28T17:33:28.242346Z"
    }
   },
   "id": "46f5838370900bf1"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "nlp = _get_spanish_parser()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-28T17:33:30.181990Z",
     "start_time": "2024-12-28T17:33:28.421715Z"
    }
   },
   "id": "6db53d9e3490b817"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "x = nlp(\"de\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-28T17:38:13.337391Z",
     "start_time": "2024-12-28T17:38:13.282501Z"
    }
   },
   "id": "4c9012b441c2f908"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "'de'"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].lemma_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-28T17:39:07.915401Z",
     "start_time": "2024-12-28T17:39:07.829232Z"
    }
   },
   "id": "f06d265e22661c81"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_tokenized == lemmatized_words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-28T17:29:11.578127Z",
     "start_time": "2024-12-28T17:29:11.528699Z"
    }
   },
   "id": "a7fed0da50a1f296"
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [],
   "source": [
    "def detokenize(tokenized_sentences: str) -> str:\n",
    "    from nltk.tokenize.treebank import TreebankWordDetokenizer as Detok\n",
    "    detokenizer = Detok()\n",
    "    text = detokenizer.detokenize(tokenized_sentences)\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-28T16:28:16.459371Z",
     "start_time": "2024-12-28T16:28:16.410928Z"
    }
   },
   "id": "94cdb856318d5054"
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "tokenized_sentences = [TreebankWordDetokenizer().detokenize(sent) for sent in lemmatized_words]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-28T16:28:16.930722Z",
     "start_time": "2024-12-28T16:28:16.872099Z"
    }
   },
   "id": "dd71ca38dd0508ad"
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detokenize(tokenized_sentences) == text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-28T16:28:17.405039Z",
     "start_time": "2024-12-28T16:28:17.351732Z"
    }
   },
   "id": "b84671dfe3d94bd9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c91b58878e48fbe1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "multilingual",
   "language": "python",
   "display_name": "multilingual"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
