{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-29T10:56:19.229336Z",
     "start_time": "2024-12-29T10:56:18.521857Z"
    }
   },
   "id": "90c99a472b487281"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import copy\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Tuple\n",
    "\n",
    "LANGUAGE = \"hebrew\"\n",
    "df = pd.read_csv(f\"/Users/itaimondshine/PycharmProjects/NLP/eval_metrics/app/app_data/cleaned/{LANGUAGE}.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-29T10:56:19.490025Z",
     "start_time": "2024-12-29T10:56:19.238224Z"
    }
   },
   "id": "e713e848c2dadb12"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "    q_idx                                              label   \n0       5  פורום ארגוני הפסיכולוגיה הציבורית מזהיר מראיינ...  \\\n1       7   בפני השופטת גרסטל באה תביעתו של הקבלן אמיליו ...   \n2       9   ב\"ישראל היום\" אולמרט צריך לעוף | ביתר העיתוני...   \n3      13  התוכנית \"תיק תקשורת\" מה-17.7.2008: על הסיקור ה...   \n4      15  את הסיפור הזה התניעו אנשי התקשורת, לא השוטרים:...   \n..    ...                                                ...   \n238   474   הפרשנים מסכימים כי אובמה לא ינקום בנתניהו | ב...   \n239   475  מה שמפליא במיוחד בעמדתו של אלון בן-דוד ביחס לפ...   \n240   494  הנהלת \"מעריב\" משיבה לעתירת ארגון העיתונאים בדב...   \n241   497  סיקור הטבח ב\"שרלי הבדו\" והתגובות לו הציבו בפני...   \n242   498  מכיבודים פוליטיים לח\"כים כדי למנוע את תמיכתם ו...   \n\n                                        gpt_prediction   \n0    פורום הארגונים למען הפסיכולוגיה הציבורית מזהיר...  \\\n1    בית-המשפט העליון פסק כי כתבה שפורסמה בעיתון \"ש...   \n2    הציטוטים העסיסיים ממפלגת העבודה מככבים הבוקר ע...   \n3    בתוכן הטקסט מתואר השפעת התקשורת על העסקה של הש...   \n4    המאמר מתאר את פרשת התקיפה על סמנכ\"לית רשת שירה...   \n..                                                 ...   \n238  כותרות העיתונים העבריים מתמקדות היום בנושאים ש...   \n239  הסיכום הוא: בן-דוד מביע נדיבות להדלפות מידע בט...   \n240  הפלט יכול להיות: \"פיטוריו של חגי מטר מ\"מעריב\" ...   \n241  העיתון \"ניו-יורק טיימס\" החליט לא לפרסם את הקרי...   \n242  הכתבה מתארת את הצורך בהקמת ועדת חקירה ממלכתית ...   \n\n                                     gemini_prediction  \n0    פורום הארגונים למען הפסיכולוגיה הציבורית קורא ...  \n1    בית המשפט העליון הפך פסיקה של בית המשפט המחוזי...  \n2    העיתונים עסקו בהתבטאויות של אהוד ברק ופואד בן ...  \n3    הפרק עוסק בסיקור התקשורתי של עסקת השבויים, תוך...  \n4    המשטרה התייחסה לתיק התקיפה של אנשי התקשורת כתי...  \n..                                                 ...  \n238  עיתוני ישראל מתמקדים בבחירת אובמה ובמשמעויותיה...  \n239  אלון בן-דוד מבקר את פרסום \"הארץ\" על חקירת ההדל...  \n240  חברת \"אחוזת הירש ישראל\", בעלת \"מעריב\", טוענת ש...  \n241  הניו-יורק טיימס החליט שלא להציג את הקריקטורה ש...  \n242  מחדלי האסון במירון ומשברים לאומיים נוספים ממחי...  \n\n[243 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>q_idx</th>\n      <th>label</th>\n      <th>gpt_prediction</th>\n      <th>gemini_prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>פורום ארגוני הפסיכולוגיה הציבורית מזהיר מראיינ...</td>\n      <td>פורום הארגונים למען הפסיכולוגיה הציבורית מזהיר...</td>\n      <td>פורום הארגונים למען הפסיכולוגיה הציבורית קורא ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>בפני השופטת גרסטל באה תביעתו של הקבלן אמיליו ...</td>\n      <td>בית-המשפט העליון פסק כי כתבה שפורסמה בעיתון \"ש...</td>\n      <td>בית המשפט העליון הפך פסיקה של בית המשפט המחוזי...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9</td>\n      <td>ב\"ישראל היום\" אולמרט צריך לעוף | ביתר העיתוני...</td>\n      <td>הציטוטים העסיסיים ממפלגת העבודה מככבים הבוקר ע...</td>\n      <td>העיתונים עסקו בהתבטאויות של אהוד ברק ופואד בן ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13</td>\n      <td>התוכנית \"תיק תקשורת\" מה-17.7.2008: על הסיקור ה...</td>\n      <td>בתוכן הטקסט מתואר השפעת התקשורת על העסקה של הש...</td>\n      <td>הפרק עוסק בסיקור התקשורתי של עסקת השבויים, תוך...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>15</td>\n      <td>את הסיפור הזה התניעו אנשי התקשורת, לא השוטרים:...</td>\n      <td>המאמר מתאר את פרשת התקיפה על סמנכ\"לית רשת שירה...</td>\n      <td>המשטרה התייחסה לתיק התקיפה של אנשי התקשורת כתי...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>238</th>\n      <td>474</td>\n      <td>הפרשנים מסכימים כי אובמה לא ינקום בנתניהו | ב...</td>\n      <td>כותרות העיתונים העבריים מתמקדות היום בנושאים ש...</td>\n      <td>עיתוני ישראל מתמקדים בבחירת אובמה ובמשמעויותיה...</td>\n    </tr>\n    <tr>\n      <th>239</th>\n      <td>475</td>\n      <td>מה שמפליא במיוחד בעמדתו של אלון בן-דוד ביחס לפ...</td>\n      <td>הסיכום הוא: בן-דוד מביע נדיבות להדלפות מידע בט...</td>\n      <td>אלון בן-דוד מבקר את פרסום \"הארץ\" על חקירת ההדל...</td>\n    </tr>\n    <tr>\n      <th>240</th>\n      <td>494</td>\n      <td>הנהלת \"מעריב\" משיבה לעתירת ארגון העיתונאים בדב...</td>\n      <td>הפלט יכול להיות: \"פיטוריו של חגי מטר מ\"מעריב\" ...</td>\n      <td>חברת \"אחוזת הירש ישראל\", בעלת \"מעריב\", טוענת ש...</td>\n    </tr>\n    <tr>\n      <th>241</th>\n      <td>497</td>\n      <td>סיקור הטבח ב\"שרלי הבדו\" והתגובות לו הציבו בפני...</td>\n      <td>העיתון \"ניו-יורק טיימס\" החליט לא לפרסם את הקרי...</td>\n      <td>הניו-יורק טיימס החליט שלא להציג את הקריקטורה ש...</td>\n    </tr>\n    <tr>\n      <th>242</th>\n      <td>498</td>\n      <td>מכיבודים פוליטיים לח\"כים כדי למנוע את תמיכתם ו...</td>\n      <td>הכתבה מתארת את הצורך בהקמת ועדת חקירה ממלכתית ...</td>\n      <td>מחדלי האסון במירון ומשברים לאומיים נוספים ממחי...</td>\n    </tr>\n  </tbody>\n</table>\n<p>243 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T13:41:08.275927Z",
     "start_time": "2024-12-27T13:41:08.254176Z"
    }
   },
   "id": "8a09ead6b4da14ce"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "sample_sumary = df.iloc[0]['gpt_prediction']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T13:41:08.281286Z",
     "start_time": "2024-12-27T13:41:08.274985Z"
    }
   },
   "id": "564db408b9091872"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'פורום הארגונים למען הפסיכולוגיה הציבורית מזהיר מפרסום תכנים טראומטיים בתקשורת, שעלולים לגרום לטראומטיזציה ולפגיעה בחוסן הנפשי של הצופים והמרואיינים. הפורום מציע קווים מנחים לראיונות עם ניצולי אירועי טראומה, ומזהיר משאלות ישירות ותיאורי זוועה. הפורום קובע כי נדרשת הקפדה על הצורך לשמור על שלומו של המרואיין ולא לגרום לנזק ממשי או להפרעה תפקודית. הפורום קורא לאיפוק וללקיחת אחריות מצד כלי התקשורת, למען בריאותם וחוסנם של האנשים.'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sumary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T13:41:08.311976Z",
     "start_time": "2024-12-27T13:41:08.281185Z"
    }
   },
   "id": "9c868617b3ad49e1"
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [],
   "source": [
    "def remove_random_words(sentences: list, num_words: int):\n",
    "    punctuation_marks = {\".\", \"?\", \"!\", '``', \"''\", ','}\n",
    "    \n",
    "    # Flatten the sentences into a list of (word, sentence_index, word_index)\n",
    "    all_words = [\n",
    "        (word, sentence_idx, word_idx)\n",
    "        for sentence_idx, sentence in enumerate(sentences)\n",
    "        for word_idx, word in enumerate(sentence)\n",
    "        if word not in punctuation_marks\n",
    "    ]\n",
    "\n",
    "    # Randomly sample words to remove\n",
    "    words_to_remove = random.sample(all_words, min(num_words, len(all_words)))\n",
    "\n",
    "    # Convert sampled words into a set of (sentence_index, word_index) for quick lookup\n",
    "    indices_to_remove = {\n",
    "        (sentence_idx, word_idx) for _, sentence_idx, word_idx in words_to_remove\n",
    "    }\n",
    "\n",
    "    # Reconstruct the sentences without the removed words\n",
    "    modified_sentences = [\n",
    "        [\n",
    "            word\n",
    "            for word_idx, word in enumerate(sentence)\n",
    "            if (sentence_idx, word_idx) not in indices_to_remove\n",
    "        ]\n",
    "        for sentence_idx, sentence in enumerate(sentences)\n",
    "    ]\n",
    "\n",
    "    return modified_sentences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-26T21:02:37.629272Z",
     "start_time": "2024-12-26T21:02:37.575922Z"
    }
   },
   "id": "5c596d833e103d45"
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "outputs": [],
   "source": [
    "def replace_with_lemma(sentences: list, num_words: int, language: str):\n",
    "    lemmatize = LANGUAGE_TO_LEMMATIZER[language]\n",
    "    punctuation_marks = {\".\", \"?\", \"!\", '``', \"''\", ','}\n",
    "    \n",
    "    # Flatten the sentences into a list of (word, sentence_index, word_index)\n",
    "    all_words = [\n",
    "        (word, sentence_idx, word_idx)\n",
    "        for sentence_idx, sentence in enumerate(sentences)\n",
    "        for word_idx, word in enumerate(sentence)\n",
    "        if word not in punctuation_marks\n",
    "    ]\n",
    "\n",
    "    # Randomly sample words to lemmatize\n",
    "    words_to_lemmatize = random.sample(all_words, min(num_words, len(all_words)))\n",
    "        # Convert sampled words into a set of (sentence_index, word_index) for quick lookup\n",
    "    \n",
    "    print(words_to_lemmatize)\n",
    "    indices_to_lemmatize = {\n",
    "        (sentence_idx, word_idx) for _, sentence_idx, word_idx in words_to_lemmatize\n",
    "    }\n",
    "    # Reconstruct the sentences with lemmatized words\n",
    "    modified_sentences = [\n",
    "            [\n",
    "                lemmatize(word) if (sentence_idx, word_idx) in indices_to_lemmatize and lemmatize(word) is not None else word\n",
    "                for word_idx, word in enumerate(sentence)\n",
    "            ]\n",
    "            for sentence_idx, sentence in enumerate(words_tokenized)\n",
    "        ]\n",
    "    return modified_sentences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T11:12:56.594177Z",
     "start_time": "2024-12-27T11:12:56.494436Z"
    }
   },
   "id": "b381d2cd30a7af6a"
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "outputs": [],
   "source": [
    "def detokenize(tokenized_sentences: str) -> str:\n",
    "    from nltk.tokenize.treebank import TreebankWordDetokenizer as Detok\n",
    "    detokenizer = Detok()\n",
    "    text = detokenizer.detokenize(tokenized_sentences)\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T11:12:56.615660Z",
     "start_time": "2024-12-27T11:12:56.551846Z"
    }
   },
   "id": "a10d260aa4cbd7c1"
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "outputs": [],
   "source": [
    "def swap_entities_in_text(text: str, ner_results: List[Tuple[str, str]], n: int) -> str:\n",
    "    \"\"\"\n",
    "    Randomly swaps N pairs of entities with the same label in the text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        ner_results (List[Tuple[str, str]]): List of tuples containing labels and entities.\n",
    "        n (int): Number of pairs to swap.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with swapped entities.\n",
    "    \"\"\"\n",
    "    # Group entities by their label\n",
    "    label_to_entities = {}\n",
    "    print(ner_results)\n",
    "    for label, entity in ner_results:\n",
    "        if label not in label_to_entities:\n",
    "            label_to_entities[label] = []\n",
    "        label_to_entities[label].append(entity)\n",
    "\n",
    "    # Identify labels with at least two entities to swap\n",
    "    swappable_labels = [label for label, entities in label_to_entities.items() if len(entities) > 1]\n",
    "\n",
    "    # Limit the number of swaps to the available swappable labels\n",
    "    n = min(n, len(swappable_labels))\n",
    "\n",
    "    # Randomly select N labels for swapping\n",
    "    selected_labels = random.sample(swappable_labels, n)\n",
    "\n",
    "    # Perform swaps\n",
    "    for label in selected_labels:\n",
    "        entities = label_to_entities[label]\n",
    "        if len(entities) >= 2:\n",
    "            entity1, entity2 = random.sample(entities, 2)\n",
    "            # Swap entity1 and entity2 in the text\n",
    "            text = text.replace(entity1, \"<TEMP>\")\n",
    "            text = text.replace(entity2, entity1)\n",
    "            text = text.replace(\"<TEMP>\", entity2)\n",
    "\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T11:12:56.683209Z",
     "start_time": "2024-12-27T11:12:56.617195Z"
    }
   },
   "id": "6e5ad514c663acd4"
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "outputs": [],
   "source": [
    "class RemoveRandomWordsConfig(BaseModel):\n",
    "    enabled: bool = Field(..., description=\"Whether random word removal is enabled.\")\n",
    "    number: int = Field(0, description=\"Number of random words to remove.\")\n",
    "class TokenizeConfig(BaseModel):\n",
    "    should_shuffle: bool = False\n",
    "    should_remove_sentence: bool = False\n",
    "    should_remove_random_words: RemoveRandomWordsConfig = Field(\n",
    "        default_factory=lambda: RemoveRandomWordsConfig(enabled=False, number=0),\n",
    "        description=\"Configuration for removing random words.\"\n",
    "    )\n",
    "    should_replace_ner: bool = False\n",
    "    should_remove_words: bool = False\n",
    "    should_replace_words_with_lemma_form: RemoveRandomWordsConfig = Field(\n",
    "    default_factory=lambda: RemoveRandomWordsConfig(enabled=False, number=0),\n",
    "    description=\"Configuration for replacing words with lemmas.\"\n",
    "    )\n",
    "    should_replace_ner_with_other: bool = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T11:32:03.455172Z",
     "start_time": "2024-12-27T11:32:03.232324Z"
    }
   },
   "id": "c0f731ec0809bfe4"
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "outputs": [],
   "source": [
    "config = TokenizeConfig(should_shuffle=True, should_remove_sentence=True, should_remove_random_words={'enabled':True, 'number': 10}, should_replace_words_with_lemma_form={'enabled':True, 'number': 10}, should_replace_ner_with_other=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T13:30:28.805579Z",
     "start_time": "2024-12-27T13:30:28.742138Z"
    }
   },
   "id": "df5a41ea3b82d39d"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydantic.deprecated'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/confection/__init__.py:38\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 38\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpydantic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv1\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseModel, Extra, ValidationError, create_model\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpydantic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv1\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfields\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ModelField\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'pydantic.v1'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mapp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapp_data\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlemmas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlemmatizers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LANGUAGE_TO_lemmatizer\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# from app.app_data.lemmas.ner import LANGUAGE_TO_NER\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/eval_metrics/app/app_data/lemmas/lemmatizers.py:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Supported Spacy pacages\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mqalsadi\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlemmatizer\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mspacy\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mzeyrek\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mapp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapp_data\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlemmas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdikta_parser\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DictaParser\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/spacy/__init__.py:6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Any, Dict, Iterable, Union\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01merrors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m setup_default_warnings\n\u001B[1;32m      8\u001B[0m setup_default_warnings()  \u001B[38;5;66;03m# noqa: E402\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# These are imported as part of the API\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/spacy/errors.py:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Literal\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mErrorsWithCodes\u001B[39;00m(\u001B[38;5;28mtype\u001B[39m):\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getattribute__\u001B[39m(\u001B[38;5;28mself\u001B[39m, code):\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/spacy/compat.py:4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mthinc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m copy_array\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcPickle\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpickle\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/thinc/__init__.py:5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mabout\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m registry\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# fmt: off\u001B[39;00m\n\u001B[1;32m      8\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mregistry\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__version__\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     11\u001B[0m ]\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/thinc/config.py:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcatalogue\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mconfection\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mconfection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m VARIABLE_RE, Config, ConfigValidationError, Promise\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtypes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Decorator\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/confection/__init__.py:42\u001B[0m\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpydantic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv1\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmain\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ModelMetaclass\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[0;32m---> 42\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpydantic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseModel, create_model, ValidationError, Extra  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpydantic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmain\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ModelMetaclass  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpydantic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfields\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ModelField  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/pydantic/__init__.py:389\u001B[0m, in \u001B[0;36m__getattr__\u001B[0;34m(attr_name)\u001B[0m\n\u001B[1;32m      0\u001B[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/importlib/__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m    124\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'pydantic.deprecated'"
     ]
    }
   ],
   "source": [
    "from app.app_data.lemmas.lemmatizers import LANGUAGE_TO_lemmatizer\n",
    "# from app.app_data.lemmas.ner import LANGUAGE_TO_NER"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-29T10:56:38.159622Z",
     "start_time": "2024-12-29T10:56:37.355473Z"
    }
   },
   "id": "78fec1a52141a93"
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "outputs": [],
   "source": [
    "LANGUAGE = 'hebrew'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T11:12:56.965728Z",
     "start_time": "2024-12-27T11:12:56.877529Z"
    }
   },
   "id": "224c1533085e2e1a"
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "def swap_entities_in_text(text: str, ner_results: List[Tuple[str, str]], n: int) -> str:\n",
    "    \"\"\"\n",
    "    Randomly swaps N pairs of entities with the same label in the text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        ner_results (List[Tuple[str, str]]): List of tuples containing labels and entities.\n",
    "        n (int): Number of pairs to swap.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with swapped entities.\n",
    "    \"\"\"\n",
    "    # Group entities by their label\n",
    "    label_to_entities = {}\n",
    "    for label, entity in ner_results:\n",
    "        if label not in label_to_entities:\n",
    "            label_to_entities[label] = []\n",
    "        label_to_entities[label].append(entity)\n",
    "\n",
    "    # Identify labels with at least two entities to swap\n",
    "    swappable_labels = [label for label, entities in label_to_entities.items() if len(entities) > 1]\n",
    "\n",
    "    # Limit the number of swaps to the available swappable labels\n",
    "    n = min(n, len(swappable_labels))\n",
    "\n",
    "    # Randomly select N labels for swapping\n",
    "    selected_labels = random.sample(swappable_labels, n)\n",
    "    \n",
    "    # Perform swaps\n",
    "    print(selected_labels)\n",
    "    for label in selected_labels:\n",
    "        entities = label_to_entities[label]\n",
    "        if len(entities) >= 2:\n",
    "            entity1, entity2 = random.sample(entities, 2)\n",
    "            # Swap entity1 and entity2 in the text\n",
    "            text = text.replace(entity1, \"<TEMP>\")\n",
    "            text = text.replace(entity2, entity1)\n",
    "            text = text.replace(\"<TEMP>\", entity2)\n",
    "\n",
    "    return text\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T13:30:32.014507Z",
     "start_time": "2024-12-27T13:30:31.953064Z"
    }
   },
   "id": "6c1c99f0a3831d6d"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def shuffle_pairs(tokenized_sentences):\n",
    "    # Iterate through the list with a step of 2 to access pairs\n",
    "    i = random.randint(-1, len(tokenized_sentences) -2)\n",
    "    tokenized_sentences[i], tokenized_sentences[i + 1] = tokenized_sentences[i + 1], tokenized_sentences[i]\n",
    "    return tokenized_sentences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-29T11:29:49.850833Z",
     "start_time": "2024-12-29T11:29:49.776406Z"
    }
   },
   "id": "d63dd12f157779ce"
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "outputs": [],
   "source": [
    "def noise_text(text: str, config: TokenizeConfig) -> str:\n",
    "    tokenized_sentences = sent_tokenize(text)\n",
    "    temp_tokenized_sentences = copy(tokenized_sentences)\n",
    "    number_of_sentences = len(tokenized_sentences)\n",
    "    \n",
    "    print(len(tokenized_sentences))\n",
    "    if config.should_shuffle:\n",
    "        tokenized_sentences = shuffle_pairs(tokenized_sentences)\n",
    "    \n",
    "    if config.should_remove_sentence: \n",
    "        tokenized_sentences.pop(random.randrange(len(tokenized_sentences))) \n",
    "        \n",
    "    if config.should_remove_random_words.enabled:\n",
    "        words_tokenized = [word_tokenize(sent) for sent in tokenized_sentences]\n",
    "        un_removed_words = remove_random_words(words_tokenized, config.should_remove_random_words.number)\n",
    "        tokenized_sentences = [TreebankWordDetokenizer().detokenize(sent) for sent in un_removed_words]\n",
    "        \n",
    "    if config.should_replace_words_with_lemma_form.enabled:\n",
    "        words_tokenized = [word_tokenize(sent) for sent in tokenized_sentences]\n",
    "        un_removed_words = replace_with_lemma(words_tokenized, config.should_replace_words_with_lemma_form.number, LANGUAGE)\n",
    "        print(f\"{un_removed_words}:\")\n",
    "        tokenized_sentences = [TreebankWordDetokenizer().detokenize(sent) for sent in un_removed_words]\n",
    "    \n",
    "    if config.should_replace_ner_with_other:\n",
    "        detokenized = detokenize(tokenized_sentences)\n",
    "        ner_results = LANGUAGE_TO_NER[LANGUAGE](detokenized)\n",
    "        text = swap_entities_in_text(detokenized, ner_results, n=3)\n",
    "    else:\n",
    "        text = detokenize(tokenized_sentences)\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T13:30:44.313820Z",
     "start_time": "2024-12-27T13:30:44.259086Z"
    }
   },
   "id": "3dd5ff5acdc4ec70"
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "outputs": [],
   "source": [
    "disrupted_dataset = []\n",
    "config = TokenizeConfig(should_shuffle=True, should_remove_sentence=True, should_remove_random_words={'enabled':True, 'number': 5}, should_replace_words_with_lemma_form=True, should_replace_ner_with_other=True)\n",
    "for idx, sample in test_df.iterrows():\n",
    "    gemini_prediction = sample['gemini_prediction']\n",
    "    gpt_prediction = sample['gpt_prediction']\n",
    "    q_idx = sample['q_idx']\n",
    "    label = sample['label']\n",
    "    strategy = random.randint(0, 2)\n",
    "    if strategy == 1:\n",
    "        gpt_prediction = noise_text(gpt_prediction, config)\n",
    "    elif strategy == 2:\n",
    "        gemini_prediction = noise_text(gemini_prediction, config)\n",
    "    new_sample = sample.to_dict() if strategy == 0 else {\n",
    "        'gemini_prediction': gemini_prediction,\n",
    "        'gpt_prediction': gpt_prediction,\n",
    "        'label': label,\n",
    "        'q_idx': q_idx\n",
    "    }\n",
    "    disrupted_dataset.append(new_sample)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T13:30:45.671389Z",
     "start_time": "2024-12-27T13:30:45.622694Z"
    }
   },
   "id": "33361254f78473ec"
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "outputs": [],
   "source": [
    "# disrupted_df = pd.DataFrame(disrupted_dataset)\n",
    "# disrupted_df.to_csv(f\"/Users/itaimondshine/PycharmProjects/NLP/eval_metrics/app/app_data/corrupted/{LANGUAGE}.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T13:30:45.903890Z",
     "start_time": "2024-12-27T13:30:45.829309Z"
    }
   },
   "id": "6accd08681266a13"
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "outputs": [],
   "source": [
    "# Build image\n",
    "# Upload"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T13:30:46.359606Z",
     "start_time": "2024-12-27T13:30:46.290067Z"
    }
   },
   "id": "c317d1794edbcb5"
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('מזהיר', 0, 5), ('של', 0, 17), ('הצופים', 0, 18), ('הארגונים', 0, 1), ('הנפשי', 0, 16), ('ולפגיעה', 0, 14), ('מפרסום', 0, 6), ('פורום', 0, 0), ('לגרום', 0, 12), ('למען', 0, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/itaimondshine/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['פורום', 'הארגונים', 'למען', 'הפסיכולוגיה', 'הציבורית', 'מזהיר', 'מפרסום', 'תכנים', 'טראומטיים', 'בתקשורת', ',', 'שעלולים', 'לגרום', 'לטראומטיזציה', 'ולפגיעה', 'בחוסן', 'הנפשי', 'של', 'הצופים', 'והמרואיינים', '.'], ['הפורום', 'מציע', 'קווים', 'מנחים', 'לראיונות', 'עם', 'ניצולי', 'אירועי', 'טראומה', ',', 'ומזהיר', 'משאלות', 'ישירות', 'ותיאורי', 'זוועה', '.'], ['הפורום', 'קובע', 'כי', 'נדרשת', 'הקפדה', 'על', 'הצורך', 'לשמור', 'על', 'שלומו', 'של', 'המרואיין', 'ולא', 'לגרום', 'לנזק', 'ממשי', 'או', 'להפרעה', 'תפקודית', '.'], ['הפורום', 'קורא', 'לאיפוק', 'וללקיחת', 'אחריות', 'מצד', 'כלי', 'התקשורת', ',', 'למען', 'בריאותם', 'וחוסנם', 'של', 'האנשים', '.']]\n"
     ]
    }
   ],
   "source": [
    "text = sample_sumary\n",
    "tokenized_sentences = sent_tokenize(text)\n",
    "words_tokenized = [word_tokenize(sent) for sent in tokenized_sentences]\n",
    "un_removed_words = replace_with_lemma([words_tokenized[0]], config.should_replace_words_with_lemma_form.number, LANGUAGE)\n",
    "\n",
    "print(words_tokenized)\n",
    "# noise_text(sample_sumary, config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T13:35:18.676724Z",
     "start_time": "2024-12-27T13:34:40.853845Z"
    }
   },
   "id": "a1cd989ecbcc8fbf"
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('הנפשי', 0, 16), ('תכנים', 0, 7), ('לגרום', 0, 12), ('הפסיכולוגיה', 0, 3), ('של', 0, 17)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/itaimondshine/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "punctuation_marks = {\".\", \"?\", \"!\", '``', \"''\", ','}\n",
    "\n",
    "lemmatize = LANGUAGE_TO_LEMMATIZER['hebrew']\n",
    "sentences = [words_tokenized[0]]\n",
    "# Flatten the sentences into a list of (word, sentence_index, word_index)\n",
    "all_words = [\n",
    "    (word, sentence_idx, word_idx)\n",
    "    for sentence_idx, sentence in enumerate(sentences)\n",
    "    for word_idx, word in enumerate(sentence)\n",
    "    if word not in punctuation_marks\n",
    "]\n",
    "\n",
    "# Randomly sample words to lemmatize\n",
    "words_to_lemmatize = random.sample(all_words, min(5, len(all_words)))\n",
    "    # Convert sampled words into a set of (sentence_index, word_index) for quick lookup\n",
    "# \n",
    "print(words_to_lemmatize)\n",
    "indices_to_lemmatize = {\n",
    "    (sentence_idx, word_idx) for _, sentence_idx, word_idx in words_to_lemmatize\n",
    "}\n",
    "# # Reconstruct the sentences with lemmatized words\n",
    "modified_sentences = [\n",
    "        [\n",
    "            lemmatize(word) if (sentence_idx, word_idx) in indices_to_lemmatize and lemmatize(word) is not None else word\n",
    "            for word_idx, word in enumerate(sentence)\n",
    "        ]\n",
    "        for sentence_idx, sentence in enumerate(words_tokenized)\n",
    "    ]\n",
    "# return modified_sentences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T13:38:24.901134Z",
     "start_time": "2024-12-27T13:38:06.538696Z"
    }
   },
   "id": "d6b5b0faf8c42fb7"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_sumary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43msample_sumary\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'sample_sumary' is not defined"
     ]
    }
   ],
   "source": [
    "sample_sumary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T13:41:02.063724Z",
     "start_time": "2024-12-27T13:41:02.001478Z"
    }
   },
   "id": "96c5a373d5f93b1b"
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "outputs": [
    {
     "data": {
      "text/plain": "[['פורום',\n  'ארגון',\n  'למען',\n  'הפסיכולוגיה',\n  'הציבורית',\n  'הזהיר',\n  'פרסום',\n  'תכנים',\n  'טראומטיים',\n  'בתקשורת',\n  ',',\n  'שעלולים',\n  'גרם',\n  'לטראומטיזציה',\n  'פגיעה',\n  'בחוסן',\n  'הנפשי',\n  'של',\n  'צופה',\n  'והמרואיינים',\n  '.'],\n ['הפורום',\n  'מציע',\n  'קווים',\n  'מנחים',\n  'לראיונות',\n  'עם',\n  'ניצולי',\n  'אירועי',\n  'טראומה',\n  ',',\n  'ומזהיר',\n  'משאלות',\n  'ישירות',\n  'ותיאורי',\n  'זוועה',\n  '.'],\n ['הפורום',\n  'קובע',\n  'כי',\n  'נדרשת',\n  'הקפדה',\n  'על',\n  'הצורך',\n  'לשמור',\n  'על',\n  'שלומו',\n  'של',\n  'המרואיין',\n  'ולא',\n  'לגרום',\n  'לנזק',\n  'ממשי',\n  'או',\n  'להפרעה',\n  'תפקודית',\n  '.'],\n ['הפורום',\n  'קורא',\n  'לאיפוק',\n  'וללקיחת',\n  'אחריות',\n  'מצד',\n  'כלי',\n  'התקשורת',\n  ',',\n  'למען',\n  'בריאותם',\n  'וחוסנם',\n  'של',\n  'האנשים',\n  '.']]"
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "un_removed_words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T13:36:02.467944Z",
     "start_time": "2024-12-27T13:36:02.332622Z"
    }
   },
   "id": "69496a79ec0a3825"
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "outputs": [
    {
     "data": {
      "text/plain": "'פורום הארגונים למען הפסיכולוגיה הציבורית מזהיר מפרסום תכנים טראומטיים בתקשורת, שעלולים לגרום לטראומטיזציה ולפגיעה בחוסן הנפשי של הצופים והמרואיינים. הפורום מציע קווים מנחים לראיונות עם ניצולי אירועי טראומה, ומזהיר משאלות ישירות ותיאורי זוועה. הפורום קובע כי נדרשת הקפדה על הצורך לשמור על שלומו של המרואיין ולא לגרום לנזק ממשי או להפרעה תפקודית. הפורום קורא לאיפוק וללקיחת אחריות מצד כלי התקשורת, למען בריאותם וחוסנם של האנשים.'"
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sumary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T13:31:30.137002Z",
     "start_time": "2024-12-27T13:31:30.014581Z"
    }
   },
   "id": "f126ae729d1514d1"
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "outputs": [],
   "source": [
    " lemmatize = LANGUAGE_TO_LEMMATIZER['hebrew']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T13:32:06.264434Z",
     "start_time": "2024-12-27T13:32:06.208153Z"
    }
   },
   "id": "b7fa702c0684f91a"
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/itaimondshine/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "'אוטומציה'"
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(\"לטראומטיזציה\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T13:32:38.214735Z",
     "start_time": "2024-12-27T13:32:33.715136Z"
    }
   },
   "id": "77ae6c8980e6850a"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qalsadi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m arabic_lemmer_chache \u001B[38;5;241m=\u001B[39m \u001B[43mqalsadi\u001B[49m\u001B[38;5;241m.\u001B[39mlemmatizer\u001B[38;5;241m.\u001B[39mLemmatizer()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'qalsadi' is not defined"
     ]
    }
   ],
   "source": [
    "arabic_lemmer_chache = qalsadi.lemmatizer.Lemmatizer()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T14:08:15.134353Z",
     "start_time": "2024-12-27T14:08:14.226600Z"
    }
   },
   "id": "74211b84ee1f5d2b"
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "outputs": [],
   "source": [
    "def replace_lemma(sentences: list, num_words: int, language: str):\n",
    "    lemmatize = LANGUAGE_TO_LEMMATIZER[language]\n",
    "    punctuation_marks = {\".\", \"?\", \"!\", '``', \"''\", ','}\n",
    "    \n",
    "    # Flatten the sentences into a list of (word, sentence_index, word_index)\n",
    "    all_words = [\n",
    "        (word, sentence_idx, word_idx)\n",
    "        for sentence_idx, sentence in enumerate(sentences)\n",
    "        for word_idx, word in enumerate(sentence)\n",
    "        if word not in punctuation_marks\n",
    "    ]\n",
    "\n",
    "    # Randomly sample words to lemmatize\n",
    "    words_to_lemmatize = random.sample(all_words, min(num_words, len(all_words)))\n",
    "    \n",
    "    print(f\"words_to_lemmatize: {len(words_to_lemmatize)}\")\n",
    "    # Convert sampled words into a set of (sentence_index, word_index) for quick lookup\n",
    "    indices_to_lemmatize = {\n",
    "        (sentence_idx, word_idx) for _, sentence_idx, word_idx in words_to_lemmatize\n",
    "    }\n",
    "    \n",
    "    # Reconstruct the sentences with lemmatized words\n",
    "    modified_sentences = [\n",
    "        [\n",
    "            lemmatize(word) if (sentence_idx, word_idx) in indices_to_lemmatize else word\n",
    "            for word_idx, word in enumerate(sentence)\n",
    "        ]\n",
    "        for sentence_idx, sentence in enumerate(sentences)\n",
    "    ]\n",
    "    return modified_sentences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T11:04:47.181640Z",
     "start_time": "2024-12-27T11:04:47.093779Z"
    }
   },
   "id": "a5fc67a6152a5440"
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "outputs": [
    {
     "data": {
      "text/plain": "'פורום הארגונים למען הפסיכולוגיה הציבורית מזהיר מפרסום תכנים טראומטיים בתקשורת, שעלולים לגרום לטראומטיזציה ולפגיעה בחוסן הנפשי של הצופים והמרואיינים. הפורום מציע קווים מנחים לראיונות עם ניצולי אירועי טראומה, ומזהיר משאלות ישירות ותיאורי זוועה. הפורום קובע כי נדרשת הקפדה על הצורך לשמור על שלומו של המרואיין ולא לגרום לנזק ממשי או להפרעה תפקודית. הפורום קורא לאיפוק וללקיחת אחריות מצד כלי התקשורת, למען בריאותם וחוסנם של האנשים.'"
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sumary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T11:04:47.510917Z",
     "start_time": "2024-12-27T11:04:47.423563Z"
    }
   },
   "id": "a0a9db6068b9a82e"
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "outputs": [],
   "source": [
    "tokenized_sentences = sent_tokenize(sample_sumary)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T11:04:47.827128Z",
     "start_time": "2024-12-27T11:04:47.740564Z"
    }
   },
   "id": "bbe1fcec7ad85dde"
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_to_lemmatize: 3\n",
      "indices: {(3, 12), (3, 13), (1, 1)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/itaimondshine/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "פורום פורום\n",
      "הארגונים ארגון\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[453], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m words_tokenized \u001B[38;5;241m=\u001B[39m [word_tokenize(sent) \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m tokenized_sentences]\n\u001B[0;32m----> 2\u001B[0m un_removed_words \u001B[38;5;241m=\u001B[39m \u001B[43mreplace_lemma\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwords_tokenized\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mLANGUAGE\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[450], line 25\u001B[0m, in \u001B[0;36mreplace_lemma\u001B[0;34m(sentences, num_words, language)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sentence_idx, sentence \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(sentences):\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m word_idx, word \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(sentence):\n\u001B[0;32m---> 25\u001B[0m         \u001B[38;5;28mprint\u001B[39m(word, \u001B[43mlemmatize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# Reconstruct the sentences with lemmatized words\u001B[39;00m\n\u001B[1;32m     30\u001B[0m modified_sentences \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     31\u001B[0m     [\n\u001B[1;32m     32\u001B[0m         lemmatize(word) \u001B[38;5;28;01mif\u001B[39;00m (sentence_idx, word_idx) \u001B[38;5;129;01min\u001B[39;00m indices_to_lemmatize \u001B[38;5;28;01melse\u001B[39;00m word\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m sentence_idx, sentence \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(sentences)\n\u001B[1;32m     36\u001B[0m ]\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/eval_metrics/app/app_data/lemmas/lemmatizers.py:54\u001B[0m, in \u001B[0;36mlemmatize_hebrew\u001B[0;34m(word)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlemmatize_hebrew\u001B[39m(word: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdikta_parser\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DictaParser\n\u001B[0;32m---> 54\u001B[0m     d \u001B[38;5;241m=\u001B[39m \u001B[43mDictaParser\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m     result_dikta \u001B[38;5;241m=\u001B[39m d\u001B[38;5;241m.\u001B[39mparse(word)\n\u001B[1;32m     56\u001B[0m     ud_trees \u001B[38;5;241m=\u001B[39m result_dikta[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mud_trees\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/eval_metrics/app/app_data/lemmas/dikta_parser.py:32\u001B[0m, in \u001B[0;36mDictaParser.__init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdicta-il/dictabert-joint\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 32\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdicta-il/dictabert-joint\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:561\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    559\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    560\u001B[0m         \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mregister(config\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m, model_class, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m--> 561\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    562\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    563\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    564\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    565\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/transformers/modeling_utils.py:3850\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   3841\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_orig \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3842\u001B[0m         torch\u001B[38;5;241m.\u001B[39mset_default_dtype(dtype_orig)\n\u001B[1;32m   3843\u001B[0m     (\n\u001B[1;32m   3844\u001B[0m         model,\n\u001B[1;32m   3845\u001B[0m         missing_keys,\n\u001B[1;32m   3846\u001B[0m         unexpected_keys,\n\u001B[1;32m   3847\u001B[0m         mismatched_keys,\n\u001B[1;32m   3848\u001B[0m         offload_index,\n\u001B[1;32m   3849\u001B[0m         error_msgs,\n\u001B[0;32m-> 3850\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_pretrained_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3851\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3852\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3853\u001B[0m \u001B[43m        \u001B[49m\u001B[43mloaded_state_dict_keys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# XXX: rename?\u001B[39;49;00m\n\u001B[1;32m   3854\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresolved_archive_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3855\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3856\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3857\u001B[0m \u001B[43m        \u001B[49m\u001B[43msharded_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msharded_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3858\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_fast_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_fast_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3859\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3860\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3861\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffload_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3862\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffload_state_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_state_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3863\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3864\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_quantized\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mquantization_method\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mQuantizationMethod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mBITS_AND_BYTES\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3865\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_modules\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_fp32_modules\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3866\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3868\u001B[0m model\u001B[38;5;241m.\u001B[39mis_loaded_in_4bit \u001B[38;5;241m=\u001B[39m load_in_4bit\n\u001B[1;32m   3869\u001B[0m model\u001B[38;5;241m.\u001B[39mis_loaded_in_8bit \u001B[38;5;241m=\u001B[39m load_in_8bit\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/transformers/modeling_utils.py:4128\u001B[0m, in \u001B[0;36mPreTrainedModel._load_pretrained_model\u001B[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001B[0m\n\u001B[1;32m   4126\u001B[0m             model\u001B[38;5;241m.\u001B[39mapply(model\u001B[38;5;241m.\u001B[39m_initialize_weights)\n\u001B[1;32m   4127\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 4128\u001B[0m         \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_initialize_weights\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4130\u001B[0m \u001B[38;5;66;03m# Set some modules to fp32 if any\u001B[39;00m\n\u001B[1;32m   4131\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m keep_in_fp32_modules \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/torch/nn/modules/module.py:728\u001B[0m, in \u001B[0;36mModule.apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    693\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001B[39;00m\n\u001B[1;32m    694\u001B[0m \u001B[38;5;124;03mas well as self. Typical use includes initializing the parameters of a model\u001B[39;00m\n\u001B[1;32m    695\u001B[0m \u001B[38;5;124;03m(see also :ref:`nn-init-doc`).\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    725\u001B[0m \n\u001B[1;32m    726\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    727\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 728\u001B[0m     \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    729\u001B[0m fn(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    730\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/torch/nn/modules/module.py:728\u001B[0m, in \u001B[0;36mModule.apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    693\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001B[39;00m\n\u001B[1;32m    694\u001B[0m \u001B[38;5;124;03mas well as self. Typical use includes initializing the parameters of a model\u001B[39;00m\n\u001B[1;32m    695\u001B[0m \u001B[38;5;124;03m(see also :ref:`nn-init-doc`).\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    725\u001B[0m \n\u001B[1;32m    726\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    727\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 728\u001B[0m     \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    729\u001B[0m fn(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    730\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/torch/nn/modules/module.py:728\u001B[0m, in \u001B[0;36mModule.apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    693\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001B[39;00m\n\u001B[1;32m    694\u001B[0m \u001B[38;5;124;03mas well as self. Typical use includes initializing the parameters of a model\u001B[39;00m\n\u001B[1;32m    695\u001B[0m \u001B[38;5;124;03m(see also :ref:`nn-init-doc`).\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    725\u001B[0m \n\u001B[1;32m    726\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    727\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 728\u001B[0m     \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    729\u001B[0m fn(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    730\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/torch/nn/modules/module.py:729\u001B[0m, in \u001B[0;36mModule.apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    727\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m    728\u001B[0m     module\u001B[38;5;241m.\u001B[39mapply(fn)\n\u001B[0;32m--> 729\u001B[0m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    730\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/transformers/modeling_utils.py:1638\u001B[0m, in \u001B[0;36mPreTrainedModel._initialize_weights\u001B[0;34m(self, module)\u001B[0m\n\u001B[1;32m   1636\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_is_hf_initialized\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m   1637\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m-> 1638\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_init_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1639\u001B[0m module\u001B[38;5;241m.\u001B[39m_is_hf_initialized \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:749\u001B[0m, in \u001B[0;36mBertPreTrainedModel._init_weights\u001B[0;34m(self, module)\u001B[0m\n\u001B[1;32m    745\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Initialize the weights\"\"\"\u001B[39;00m\n\u001B[1;32m    746\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(module, nn\u001B[38;5;241m.\u001B[39mLinear):\n\u001B[1;32m    747\u001B[0m     \u001B[38;5;66;03m# Slightly different from the TF version which uses truncated_normal for initialization\u001B[39;00m\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;66;03m# cf https://github.com/pytorch/pytorch/pull/5617\u001B[39;00m\n\u001B[0;32m--> 749\u001B[0m     \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormal_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmean\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstd\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minitializer_range\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    750\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m module\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    751\u001B[0m         module\u001B[38;5;241m.\u001B[39mbias\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mzero_()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "words_tokenized = [word_tokenize(sent) for sent in tokenized_sentences]\n",
    "un_removed_words = replace_lemma(words_tokenized, 3, LANGUAGE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T11:04:57.327461Z",
     "start_time": "2024-12-27T11:04:48.042441Z"
    }
   },
   "id": "aa3eb02e42a230ad"
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "outputs": [],
   "source": [
    "    lemmatize = LANGUAGE_TO_LEMMATIZER['hebrew']\n",
    "    punctuation_marks = {\".\", \"?\", \"!\", '``', \"''\", ','}\n",
    "    \n",
    "    # Flatten the sentences into a list of (word, sentence_index, word_index)\n",
    "    all_words = [\n",
    "        (word, sentence_idx, word_idx)\n",
    "        for sentence_idx, sentence in enumerate(words_tokenized)\n",
    "        for word_idx, word in enumerate(sentence)\n",
    "        if word not in punctuation_marks\n",
    "    ]\n",
    "\n",
    "    # Randomly sample words to lemmatize\n",
    "    words_to_lemmatize = random.sample(all_words, min(5, len(all_words)))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T11:05:15.404462Z",
     "start_time": "2024-12-27T11:05:15.316310Z"
    }
   },
   "id": "c35651f5ee45f914"
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "outputs": [
    {
     "data": {
      "text/plain": "[('להפרעה', 2, 17),\n ('והמרואיינים', 0, 19),\n ('הצורך', 2, 6),\n ('הפורום', 3, 0),\n ('אחריות', 3, 4)]"
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_lemmatize"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T11:05:18.031649Z",
     "start_time": "2024-12-27T11:05:17.789898Z"
    }
   },
   "id": "caa4183854c535d"
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_to_lemmatize: 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"words_to_lemmatize: {len(words_to_lemmatize)}\")\n",
    "# Convert sampled words into a set of (sentence_index, word_index) for quick lookup\n",
    "indices_to_lemmatize = {\n",
    "    (sentence_idx, word_idx) for _, sentence_idx, word_idx in words_to_lemmatize\n",
    "}\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T11:05:45.872298Z",
     "start_time": "2024-12-27T11:05:45.823024Z"
    }
   },
   "id": "a64a46cba0eca5d3"
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "outputs": [
    {
     "data": {
      "text/plain": "{(0, 19), (2, 6), (2, 17), (3, 0), (3, 4)}"
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_to_lemmatize"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T11:05:47.893208Z",
     "start_time": "2024-12-27T11:05:47.733406Z"
    }
   },
   "id": "22cf32deaff9ec0b"
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/itaimondshine/PycharmProjects/NLP/Evaluation-Of-Unrepresented-Languages/multilingual/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    " modified_sentences = [\n",
    "        [\n",
    "            lemmatize(word) if (sentence_idx, word_idx) in indices_to_lemmatize and lemmatize(word) is not None else word\n",
    "            for word_idx, word in enumerate(sentence)\n",
    "        ]\n",
    "        for sentence_idx, sentence in enumerate(words_tokenized)\n",
    "    ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T11:08:52.424994Z",
     "start_time": "2024-12-27T11:08:23.863409Z"
    }
   },
   "id": "1d48cbda6799377e"
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "outputs": [
    {
     "data": {
      "text/plain": "[['פורום',\n  'הארגונים',\n  'למען',\n  'הפסיכולוגיה',\n  'הציבורית',\n  'מזהיר',\n  'מפרסום',\n  'תכנים',\n  'טראומטיים',\n  'בתקשורת',\n  ',',\n  'שעלולים',\n  'לגרום',\n  'לטראומטיזציה',\n  'ולפגיעה',\n  'בחוסן',\n  'הנפשי',\n  'של',\n  'הצופים',\n  'מרואיין',\n  '.'],\n ['הפורום',\n  'מציע',\n  'קווים',\n  'מנחים',\n  'לראיונות',\n  'עם',\n  'ניצולי',\n  'אירועי',\n  'טראומה',\n  ',',\n  'ומזהיר',\n  'משאלות',\n  'ישירות',\n  'ותיאורי',\n  'זוועה',\n  '.'],\n ['הפורום',\n  'קובע',\n  'כי',\n  'נדרשת',\n  'הקפדה',\n  'על',\n  'צורך',\n  'לשמור',\n  'על',\n  'שלומו',\n  'של',\n  'המרואיין',\n  'ולא',\n  'לגרום',\n  'לנזק',\n  'ממשי',\n  'או',\n  'הפרעה',\n  'תפקודית',\n  '.'],\n ['פורום',\n  'קורא',\n  'לאיפוק',\n  'וללקיחת',\n  'אחריות',\n  'מצד',\n  'כלי',\n  'התקשורת',\n  ',',\n  'למען',\n  'בריאותם',\n  'וחוסנם',\n  'של',\n  'האנשים',\n  '.']]"
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_sentences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-27T11:08:52.491677Z",
     "start_time": "2024-12-27T11:08:52.430032Z"
    }
   },
   "id": "a40217ca8b5ccdcf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4f7581ff0ac8620f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "multilingual",
   "language": "python",
   "display_name": "multilingual"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
